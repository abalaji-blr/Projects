---
typora-copy-images-to: ../QuickReference
---

# Generative Modeling & Generative Adverserial Networks (GAN)

1. **What is Generative Modeling?**

Given a set of training samples, the model can form / identify  the probability distribution, which explains where the training examples came from.

The Generative modeling can be used in the following ways:

 * Density estimation

   From the samples, Identify the probability density estimation.

 * Sample Generation.

   From the samples, learn the probablity distribution and generate the sample using the distribution.

   GAN comes under this category.



2. **Why Generative Models are important?**

   * Ability to use compilcated probability distribution.

   * Used to simulate possible scenarios in the case of Reinforced learning.

   * Fill the missing data.

   * Realistic generation tasks. ( super ressolution of images)

   * Multi modal outputs

     * one test sample can be mapped to multiple outputs.
     * Image to Image translation. Turning sketches to images.

     

3. How do **generative models** work?

   * Maximum Likelihood Probablity Distribution
     * Explicit Density Function
       * Tractable Density
       * Approximate Density
         * Variational (Varitonal Autoencoders)
         * Markov Chain (Boltzmann Machine)
     * Implicit Density Function
       * Markov Chain (GSN - Generative Sotchastic Network)
       * Direct sampling (GAN - Generative Sampling Network)

4. How do GAN's work?

   [AI wiki | GAN Blog](<https://skymind.ai/wiki/generative-adversarial-network-gan>)

   * There are two different models (networks) which are adversaries to each other.

     * Generator
       * A Fully Connected Neural Network
       * They generate the samples which are intended to be in the *training distribution*.
     * Discriminator
       * Another Fully connected Neural Network
       * Need to train this model using the training samples.
       * Also, it examines the *generated samples* (from generator) with the training set and classify whether the sample is *fake or real*. 

     ![Screenshot 2019-07-30 16.03.31](/Users/abalaji/Documents/GitProjects/Projects/QuickReference/gan.png)

     

5. What are *Autoencoders*?

   Autoencoders are family of neural networks for which the input is same as the output. They work by **compressing** the input into a **latent-space representation**, and then reconstructing the output from this representation.

   It is a pair of two connected networks, an encoder and a decoder.

   [More info](<https://ai-odyssey.com/2017/02/07/autoencoders%E2%80%8A/>)

   ![Screenshot 2019-07-30 16.49.08](/Users/abalaji/Documents/GitProjects/Projects/QuickReference/autoencoder.png)

6. What are Variational Autoencoders?

   The Variational Autoencoders also have two connected networks, an encoder and a decoder.

   They have a property that the **latent space** generated by encoder by design is **continuous**.

   Thus allows easy random sampling. **They achieve by not just storing the encoding vector of size n,**

   **instead they store - a vector of means, $\mu$  and a vector of variance, $\sigmaâ€‹$. **

   [Blog](<https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf>)

   

7. What is information **entropy**?

   In information theory, entropy is measurement unit (aka *bits*) to carry information without any loss from source to destination. It represents the minimum number of bits required for the information.

   It directly depends on the *probability*  of the information and number of decisions to select that information (from tree structure).

   So, entropy is $ H = p * log(1/p)$. It can also be rewritten as $ H = -p * log(p) $.

   See wikipedia / khanacademy for more info.

   

8. What is **KL (Kullback-Liebler) Divergence**?

   KL Divergence measures one probability distribution with another (reference probability distribution).

   It is also called as *relative entropy*. When **KL Divergence is zero, both the distribution are identical.**

   

9. 

10. What is DCGAN ( Deep Convolutional GAN)?

    

### Blogs

* [OpenAI | Generative Models](<https://openai.com/blog/generative-models/>)
* [AI Odyssey](<https://ai-odyssey.com/2017/02/24/latent-space-visualization%e2%80%8a/>)
* [Skymind ](<https://skymind.ai/wiki/variational-autoencoder>)
* 





