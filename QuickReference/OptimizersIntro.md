[TOC]

# About Optimizers and their parameters

1) GD (Gradient Discent)
2) SGD (Stochastic Gradient Discent)
3) Using Adaptive Learning Rate:
    * Adam
    * RMSProp

## SGD Parameters

* lr (Learning Rate)
* momentum 
* decay
* nesterov


