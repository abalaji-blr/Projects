{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_GAN_v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qw2Dg35u4Gi",
        "colab_type": "text"
      },
      "source": [
        "# Generative Adversarial Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juTbeEOtu-LR",
        "colab_type": "text"
      },
      "source": [
        "**Objective**:\n",
        "Build a Generative Adversarial network for MNIST images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxqPEGmwuybm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNjaXb4Pvm93",
        "colab_type": "text"
      },
      "source": [
        "## Define GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF2CASmAx1A4",
        "colab_type": "code",
        "outputId": "a05e3fe2-7f6c-4ea5-b8c8-2fc8fa9734d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrBmHbde-8io",
        "colab_type": "text"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA2DblDxwbLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " (X_train, X_test), (Y_train, Y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJPK9xUi_Xef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note that for GAN, we are going to use only the train images. No labels are required.\n",
        "# Our objective of the discriminator is to find out whether the image is fake or real.\n",
        "#X_train[0]\n",
        "\n",
        "# rescale the images : -1 to 1 instead of 0 to 255\n",
        "\n",
        "X_train = X_train / 127.5 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3gLKFyE_0Vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyOOelqvATgf",
        "colab_type": "code",
        "outputId": "855aeb7e-3a6a-46f9-d48c-842115855f08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tMdcK3qgFZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "?np.expand_dims"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BdkHCzQAbvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.expand_dims(X_train, axis=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guRujP5J-WNn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3009c73b-1a2b-4017-ac89-11e001881144"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvSQEe7R5iDC",
        "colab_type": "text"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7uzhEOh5mWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MNIST image details\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "\n",
        "## Latent Space (vectors) dimension\n",
        "latent_dim = 100\n",
        "\n",
        "# optimizer\n",
        "optimizer = Adam(0.0002, 0.5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_2wFoBx5LpX",
        "colab_type": "text"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3JOOyqt5P8L",
        "colab_type": "text"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sivHP8i4omr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Fully connected network\n",
        "##  inputs: image -> output: fake/real status\n",
        "def build_discriminator():\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Flatten(input_shape=img_shape))\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  # output is fake / real\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  input = Input(shape=img_shape)\n",
        "  out = model(input)\n",
        "  # use Model Functional API\n",
        "  return Model(inputs=input, outputs=out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH3v64A75ad7",
        "colab_type": "text"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pe6sXOs5Zqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## generator \n",
        "#\n",
        "# Fully conected network\n",
        "#\n",
        "##  inputs: latent space -> output: image\n",
        "def build_generator():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Dense(256, input_dim=latent_dim))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(1024))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  # create the image\n",
        "  model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "  model.add(Reshape(img_shape))\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  # input is noise\n",
        "  input = Input(shape=(latent_dim,))\n",
        "  out = model(input)\n",
        "\n",
        "  return Model(inputs=input, outputs=out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1JdsTwx6xRt",
        "colab_type": "text"
      },
      "source": [
        "## Build GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USV3FH_G60KC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "9e7ef762-d195-4c40-976c-a503361dce49"
      },
      "source": [
        "# build and compile discriminator\n",
        "# \n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "                            optimizer=optimizer,\n",
        "                            metrics=['accuracy'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0801 11:24:31.697007 139920668424064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0801 11:24:31.711242 139920668424064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0801 11:24:31.781128 139920668424064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0801 11:24:31.788537 139920668424064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0801 11:24:31.794745 139920668424064 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIFCQO_K7kjn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "48ac17a9-fbb3-4b85-b238-e0b3c29b1ad2"
      },
      "source": [
        "## build generator\n",
        "generator = build_generator()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0801 11:30:46.263050 139920668424064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 784)               803600    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7iXipBz8w_R",
        "colab_type": "text"
      },
      "source": [
        "### Combined Model - GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_OZsf917QbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The generator takes noise as input and generates imgs\n",
        "z = Input(shape=(latent_dim,))\n",
        "img = generator(z)\n",
        "\n",
        "# For the combined model we will only train the generator\n",
        "discriminator.trainable = False\n",
        "\n",
        "# The discriminator takes generated images as input and determines validity\n",
        "validity = discriminator(img)\n",
        "\n",
        "# The combined model  (stacked generator and discriminator)\n",
        "# Trains the generator to fool the discriminator\n",
        "combined = Model(z, validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89u1RCWE9DL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "3dc94dff-c5fb-407b-fbc0-92b5d367a398"
      },
      "source": [
        "combined.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "model_2 (Model)              (None, 28, 28, 1)         1493520   \n",
            "_________________________________________________________________\n",
            "model_1 (Model)              (None, 1)                 533505    \n",
            "=================================================================\n",
            "Total params: 2,027,025\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 537,089\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsdLrABw9qph",
        "colab_type": "text"
      },
      "source": [
        "## Training GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKJDWZS7-fK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 30000\n",
        "batch_size = 32\n",
        "sample_interval = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDILfqmO_vLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/images1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uCDG0x-_BSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the samples\n",
        "# As training was done, check the generated images at regular intervals.\n",
        "#\n",
        "def sample_images(epoch):\n",
        "  r, c = 5, 5\n",
        "  noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "  gen_imgs = generator.predict(noise)\n",
        "\n",
        "  # Rescale images 0 - 1\n",
        "  gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  cnt = 0\n",
        "  for i in range(r):\n",
        "    for j in range(c):\n",
        "      axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "      axs[i,j].axis('off')\n",
        "      cnt += 1\n",
        "  fig.savefig(\"/content/images1/%d.png\" % epoch)\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jOf0F449qJT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb41fe2e-462c-4c33-d550-063667cf0efa"
      },
      "source": [
        "# build Adversarial ground truths for discriminator and generator\n",
        "valid = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))\n",
        "\n",
        "# Train both the models.\n",
        "#\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  #--------------------\n",
        "  # train discriminator\n",
        "  #--------------------\n",
        "\n",
        "  # Select a random batch of images\n",
        "  idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "  imgs = X_train[idx]\n",
        "\n",
        "  # Generate a batch of new images, using generator\n",
        "  noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "  gen_imgs = generator.predict(noise)\n",
        "\n",
        "  # train generator using both real and fake images.\n",
        "  d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "  d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "  d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "  #-------------\n",
        "  # train generator\n",
        "  #-----------------\n",
        "  noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "  # Train the generator (to have the discriminator label samples as valid)\n",
        "  g_loss = combined.train_on_batch(noise, valid)\n",
        "\n",
        "\n",
        "\n",
        "  # If at save interval => save generated image samples\n",
        "  if epoch % sample_interval == 0:\n",
        "    # Plot the progress\n",
        "    print (\"Epoch %d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "    sample_images(epoch)\n",
        "        "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 [D loss: 0.643345, acc.: 67.19%] [G loss: 0.861467]\n",
            "Epoch 200 [D loss: 0.608127, acc.: 71.88%] [G loss: 0.771584]\n",
            "Epoch 400 [D loss: 0.586186, acc.: 79.69%] [G loss: 0.942852]\n",
            "Epoch 600 [D loss: 0.600886, acc.: 64.06%] [G loss: 0.915870]\n",
            "Epoch 800 [D loss: 0.523256, acc.: 81.25%] [G loss: 1.014366]\n",
            "Epoch 1000 [D loss: 0.506490, acc.: 82.81%] [G loss: 1.032790]\n",
            "Epoch 1200 [D loss: 0.591494, acc.: 64.06%] [G loss: 0.933244]\n",
            "Epoch 1400 [D loss: 0.604301, acc.: 64.06%] [G loss: 0.959686]\n",
            "Epoch 1600 [D loss: 0.628410, acc.: 71.88%] [G loss: 1.012214]\n",
            "Epoch 1800 [D loss: 0.609851, acc.: 70.31%] [G loss: 0.984196]\n",
            "Epoch 2000 [D loss: 0.591792, acc.: 65.62%] [G loss: 0.963633]\n",
            "Epoch 2200 [D loss: 0.550544, acc.: 73.44%] [G loss: 1.015582]\n",
            "Epoch 2400 [D loss: 0.560318, acc.: 75.00%] [G loss: 0.980694]\n",
            "Epoch 2600 [D loss: 0.553365, acc.: 75.00%] [G loss: 1.000778]\n",
            "Epoch 2800 [D loss: 0.558361, acc.: 75.00%] [G loss: 1.035558]\n",
            "Epoch 3000 [D loss: 0.607478, acc.: 70.31%] [G loss: 0.991809]\n",
            "Epoch 3200 [D loss: 0.583620, acc.: 73.44%] [G loss: 1.020634]\n",
            "Epoch 3400 [D loss: 0.638440, acc.: 54.69%] [G loss: 1.086882]\n",
            "Epoch 3600 [D loss: 0.607738, acc.: 64.06%] [G loss: 0.907718]\n",
            "Epoch 3800 [D loss: 0.603908, acc.: 70.31%] [G loss: 0.955862]\n",
            "Epoch 4000 [D loss: 0.651649, acc.: 64.06%] [G loss: 0.994332]\n",
            "Epoch 4200 [D loss: 0.644554, acc.: 65.62%] [G loss: 0.924652]\n",
            "Epoch 4400 [D loss: 0.668288, acc.: 60.94%] [G loss: 1.018584]\n",
            "Epoch 4600 [D loss: 0.627392, acc.: 60.94%] [G loss: 0.925630]\n",
            "Epoch 4800 [D loss: 0.615522, acc.: 68.75%] [G loss: 0.950691]\n",
            "Epoch 5000 [D loss: 0.631368, acc.: 65.62%] [G loss: 0.972287]\n",
            "Epoch 5200 [D loss: 0.645221, acc.: 64.06%] [G loss: 0.920740]\n",
            "Epoch 5400 [D loss: 0.597581, acc.: 67.19%] [G loss: 0.900621]\n",
            "Epoch 5600 [D loss: 0.612149, acc.: 67.19%] [G loss: 0.965112]\n",
            "Epoch 5800 [D loss: 0.656293, acc.: 67.19%] [G loss: 0.974579]\n",
            "Epoch 6000 [D loss: 0.618525, acc.: 64.06%] [G loss: 0.920193]\n",
            "Epoch 6200 [D loss: 0.606734, acc.: 70.31%] [G loss: 0.891343]\n",
            "Epoch 6400 [D loss: 0.647942, acc.: 62.50%] [G loss: 0.990023]\n",
            "Epoch 6600 [D loss: 0.626742, acc.: 59.38%] [G loss: 0.969141]\n",
            "Epoch 6800 [D loss: 0.606936, acc.: 64.06%] [G loss: 0.980801]\n",
            "Epoch 7000 [D loss: 0.647245, acc.: 67.19%] [G loss: 0.990117]\n",
            "Epoch 7200 [D loss: 0.633469, acc.: 65.62%] [G loss: 0.968800]\n",
            "Epoch 7400 [D loss: 0.698782, acc.: 54.69%] [G loss: 1.123836]\n",
            "Epoch 7600 [D loss: 0.579679, acc.: 71.88%] [G loss: 0.990615]\n",
            "Epoch 7800 [D loss: 0.645712, acc.: 64.06%] [G loss: 1.004503]\n",
            "Epoch 8000 [D loss: 0.611316, acc.: 59.38%] [G loss: 0.951314]\n",
            "Epoch 8200 [D loss: 0.624019, acc.: 64.06%] [G loss: 1.109714]\n",
            "Epoch 8400 [D loss: 0.617220, acc.: 57.81%] [G loss: 0.926507]\n",
            "Epoch 8600 [D loss: 0.665038, acc.: 57.81%] [G loss: 0.911485]\n",
            "Epoch 8800 [D loss: 0.751510, acc.: 54.69%] [G loss: 1.000399]\n",
            "Epoch 9000 [D loss: 0.544286, acc.: 79.69%] [G loss: 1.022699]\n",
            "Epoch 9200 [D loss: 0.625038, acc.: 67.19%] [G loss: 0.840340]\n",
            "Epoch 9400 [D loss: 0.670257, acc.: 54.69%] [G loss: 1.028137]\n",
            "Epoch 9600 [D loss: 0.498519, acc.: 78.12%] [G loss: 0.973554]\n",
            "Epoch 9800 [D loss: 0.671850, acc.: 59.38%] [G loss: 1.024014]\n",
            "Epoch 10000 [D loss: 0.705757, acc.: 54.69%] [G loss: 0.991713]\n",
            "Epoch 10200 [D loss: 0.612217, acc.: 62.50%] [G loss: 1.031484]\n",
            "Epoch 10400 [D loss: 0.672321, acc.: 57.81%] [G loss: 0.861467]\n",
            "Epoch 10600 [D loss: 0.719280, acc.: 62.50%] [G loss: 1.014374]\n",
            "Epoch 10800 [D loss: 0.642208, acc.: 59.38%] [G loss: 0.966039]\n",
            "Epoch 11000 [D loss: 0.644441, acc.: 54.69%] [G loss: 1.000259]\n",
            "Epoch 11200 [D loss: 0.666888, acc.: 57.81%] [G loss: 1.052555]\n",
            "Epoch 11400 [D loss: 0.651199, acc.: 60.94%] [G loss: 0.907004]\n",
            "Epoch 11600 [D loss: 0.714988, acc.: 51.56%] [G loss: 0.807040]\n",
            "Epoch 11800 [D loss: 0.637769, acc.: 64.06%] [G loss: 0.957997]\n",
            "Epoch 12000 [D loss: 0.665039, acc.: 60.94%] [G loss: 0.976930]\n",
            "Epoch 12200 [D loss: 0.634189, acc.: 59.38%] [G loss: 0.974060]\n",
            "Epoch 12400 [D loss: 0.620345, acc.: 68.75%] [G loss: 0.980491]\n",
            "Epoch 12600 [D loss: 0.652670, acc.: 54.69%] [G loss: 0.943278]\n",
            "Epoch 12800 [D loss: 0.604004, acc.: 65.62%] [G loss: 0.910241]\n",
            "Epoch 13000 [D loss: 0.588178, acc.: 70.31%] [G loss: 0.873078]\n",
            "Epoch 13200 [D loss: 0.639802, acc.: 64.06%] [G loss: 1.025698]\n",
            "Epoch 13400 [D loss: 0.704183, acc.: 50.00%] [G loss: 0.940604]\n",
            "Epoch 13600 [D loss: 0.613126, acc.: 68.75%] [G loss: 1.024091]\n",
            "Epoch 13800 [D loss: 0.647658, acc.: 68.75%] [G loss: 1.040549]\n",
            "Epoch 14000 [D loss: 0.624492, acc.: 64.06%] [G loss: 1.054245]\n",
            "Epoch 14200 [D loss: 0.615645, acc.: 70.31%] [G loss: 0.917464]\n",
            "Epoch 14400 [D loss: 0.551825, acc.: 73.44%] [G loss: 0.945409]\n",
            "Epoch 14600 [D loss: 0.571808, acc.: 76.56%] [G loss: 1.007477]\n",
            "Epoch 14800 [D loss: 0.620234, acc.: 70.31%] [G loss: 1.018546]\n",
            "Epoch 15000 [D loss: 0.641837, acc.: 62.50%] [G loss: 0.899177]\n",
            "Epoch 15200 [D loss: 0.657897, acc.: 50.00%] [G loss: 0.984035]\n",
            "Epoch 15400 [D loss: 0.640784, acc.: 64.06%] [G loss: 0.987517]\n",
            "Epoch 15600 [D loss: 0.653520, acc.: 57.81%] [G loss: 1.056255]\n",
            "Epoch 15800 [D loss: 0.689361, acc.: 59.38%] [G loss: 0.985975]\n",
            "Epoch 16000 [D loss: 0.604295, acc.: 68.75%] [G loss: 0.925516]\n",
            "Epoch 16200 [D loss: 0.677936, acc.: 59.38%] [G loss: 0.923346]\n",
            "Epoch 16400 [D loss: 0.628954, acc.: 65.62%] [G loss: 1.055720]\n",
            "Epoch 16600 [D loss: 0.554924, acc.: 71.88%] [G loss: 0.967054]\n",
            "Epoch 16800 [D loss: 0.646245, acc.: 68.75%] [G loss: 0.981292]\n",
            "Epoch 17000 [D loss: 0.612202, acc.: 62.50%] [G loss: 1.122576]\n",
            "Epoch 17200 [D loss: 0.566004, acc.: 78.12%] [G loss: 0.962233]\n",
            "Epoch 17400 [D loss: 0.658547, acc.: 59.38%] [G loss: 1.046938]\n",
            "Epoch 17600 [D loss: 0.598828, acc.: 71.88%] [G loss: 0.960946]\n",
            "Epoch 17800 [D loss: 0.581862, acc.: 68.75%] [G loss: 0.981257]\n",
            "Epoch 18000 [D loss: 0.646243, acc.: 62.50%] [G loss: 1.114469]\n",
            "Epoch 18200 [D loss: 0.599508, acc.: 71.88%] [G loss: 1.027210]\n",
            "Epoch 18400 [D loss: 0.621905, acc.: 64.06%] [G loss: 1.040352]\n",
            "Epoch 18600 [D loss: 0.648460, acc.: 60.94%] [G loss: 0.974600]\n",
            "Epoch 18800 [D loss: 0.676937, acc.: 53.12%] [G loss: 0.992212]\n",
            "Epoch 19000 [D loss: 0.646553, acc.: 62.50%] [G loss: 0.939675]\n",
            "Epoch 19200 [D loss: 0.682342, acc.: 59.38%] [G loss: 1.027110]\n",
            "Epoch 19400 [D loss: 0.626922, acc.: 60.94%] [G loss: 0.900663]\n",
            "Epoch 19600 [D loss: 0.601016, acc.: 68.75%] [G loss: 0.867033]\n",
            "Epoch 19800 [D loss: 0.657830, acc.: 57.81%] [G loss: 1.131823]\n",
            "Epoch 20000 [D loss: 0.663053, acc.: 57.81%] [G loss: 1.064700]\n",
            "Epoch 20200 [D loss: 0.658638, acc.: 62.50%] [G loss: 0.876028]\n",
            "Epoch 20400 [D loss: 0.717766, acc.: 54.69%] [G loss: 1.128407]\n",
            "Epoch 20600 [D loss: 0.581471, acc.: 62.50%] [G loss: 1.084102]\n",
            "Epoch 20800 [D loss: 0.675525, acc.: 65.62%] [G loss: 0.954607]\n",
            "Epoch 21000 [D loss: 0.616665, acc.: 68.75%] [G loss: 1.035491]\n",
            "Epoch 21200 [D loss: 0.680425, acc.: 56.25%] [G loss: 0.929450]\n",
            "Epoch 21400 [D loss: 0.587878, acc.: 68.75%] [G loss: 1.086800]\n",
            "Epoch 21600 [D loss: 0.707212, acc.: 53.12%] [G loss: 1.128111]\n",
            "Epoch 21800 [D loss: 0.566691, acc.: 71.88%] [G loss: 1.110003]\n",
            "Epoch 22000 [D loss: 0.657696, acc.: 53.12%] [G loss: 1.196589]\n",
            "Epoch 22200 [D loss: 0.565606, acc.: 73.44%] [G loss: 0.995322]\n",
            "Epoch 22400 [D loss: 0.638946, acc.: 64.06%] [G loss: 1.010985]\n",
            "Epoch 22600 [D loss: 0.689579, acc.: 53.12%] [G loss: 1.058357]\n",
            "Epoch 22800 [D loss: 0.698108, acc.: 68.75%] [G loss: 1.040660]\n",
            "Epoch 23000 [D loss: 0.701556, acc.: 54.69%] [G loss: 1.097942]\n",
            "Epoch 23200 [D loss: 0.628019, acc.: 62.50%] [G loss: 1.081865]\n",
            "Epoch 23400 [D loss: 0.590627, acc.: 64.06%] [G loss: 1.103867]\n",
            "Epoch 23600 [D loss: 0.583288, acc.: 68.75%] [G loss: 1.030828]\n",
            "Epoch 23800 [D loss: 0.655760, acc.: 59.38%] [G loss: 1.023750]\n",
            "Epoch 24000 [D loss: 0.561623, acc.: 76.56%] [G loss: 0.986472]\n",
            "Epoch 24200 [D loss: 0.696320, acc.: 59.38%] [G loss: 1.080510]\n",
            "Epoch 24400 [D loss: 0.631073, acc.: 62.50%] [G loss: 0.953371]\n",
            "Epoch 24600 [D loss: 0.576933, acc.: 73.44%] [G loss: 1.155010]\n",
            "Epoch 24800 [D loss: 0.622860, acc.: 73.44%] [G loss: 1.033264]\n",
            "Epoch 25000 [D loss: 0.688873, acc.: 57.81%] [G loss: 1.016602]\n",
            "Epoch 25200 [D loss: 0.631830, acc.: 60.94%] [G loss: 1.008255]\n",
            "Epoch 25400 [D loss: 0.589604, acc.: 62.50%] [G loss: 1.155036]\n",
            "Epoch 25600 [D loss: 0.609580, acc.: 67.19%] [G loss: 1.052960]\n",
            "Epoch 25800 [D loss: 0.568123, acc.: 73.44%] [G loss: 1.120474]\n",
            "Epoch 26000 [D loss: 0.619022, acc.: 64.06%] [G loss: 0.999949]\n",
            "Epoch 26200 [D loss: 0.623987, acc.: 67.19%] [G loss: 0.993763]\n",
            "Epoch 26400 [D loss: 0.558136, acc.: 67.19%] [G loss: 0.990213]\n",
            "Epoch 26600 [D loss: 0.696447, acc.: 60.94%] [G loss: 1.125438]\n",
            "Epoch 26800 [D loss: 0.641811, acc.: 73.44%] [G loss: 1.055541]\n",
            "Epoch 27000 [D loss: 0.600259, acc.: 65.62%] [G loss: 1.110637]\n",
            "Epoch 27200 [D loss: 0.641621, acc.: 64.06%] [G loss: 1.001513]\n",
            "Epoch 27400 [D loss: 0.705450, acc.: 56.25%] [G loss: 0.992128]\n",
            "Epoch 27600 [D loss: 0.593753, acc.: 70.31%] [G loss: 1.215346]\n",
            "Epoch 27800 [D loss: 0.655734, acc.: 56.25%] [G loss: 0.967535]\n",
            "Epoch 28000 [D loss: 0.664158, acc.: 60.94%] [G loss: 0.948627]\n",
            "Epoch 28200 [D loss: 0.583889, acc.: 62.50%] [G loss: 1.164277]\n",
            "Epoch 28400 [D loss: 0.586382, acc.: 70.31%] [G loss: 0.901359]\n",
            "Epoch 28600 [D loss: 0.601118, acc.: 60.94%] [G loss: 1.114780]\n",
            "Epoch 28800 [D loss: 0.587332, acc.: 68.75%] [G loss: 1.008247]\n",
            "Epoch 29000 [D loss: 0.551495, acc.: 76.56%] [G loss: 0.976431]\n",
            "Epoch 29200 [D loss: 0.620930, acc.: 68.75%] [G loss: 0.973791]\n",
            "Epoch 29400 [D loss: 0.597398, acc.: 70.31%] [G loss: 1.075636]\n",
            "Epoch 29600 [D loss: 0.639221, acc.: 67.19%] [G loss: 1.057763]\n",
            "Epoch 29800 [D loss: 0.669082, acc.: 57.81%] [G loss: 0.899849]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU5jxAxihJYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}