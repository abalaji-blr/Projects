{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN_Horses2Zebra_v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yjj8Z45THII",
        "colab_type": "text"
      },
      "source": [
        "# CycleGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D3Ax09wTTN2",
        "colab_type": "text"
      },
      "source": [
        "**Objective:** Implement CycleGAN for horses2zebra dataset\n",
        "\n",
        "**Paper**:\n",
        "[Unpaired Image to Image Translation using Cycle-Consistent GAN](https://arxiv.org/pdf/1703.10593.pdf)\n",
        "\n",
        "Some important points to note about the CycleGAN:\n",
        "* The goal is to learn mapping functions between two domains X & Y.\n",
        "*  Model includes the following things:\n",
        "  * Mapping G: X -> Y\n",
        "  * Mapping F : Y -> X\n",
        "  * Two Adversarial Discriminators:\n",
        "    * $D_x$ to distinguish between images {x} and generated image {F{y}}\n",
        "    * $D_y$ to distinguish between y and generated image {G{x}}\n",
        "    \n",
        "* Objective contains the following losses:\n",
        "  * **Adversarial loss:** matching the distribution of the generated images to the data distribution of the target domain.\n",
        "  \n",
        "  * **Cycle-consistency loss**: to prevent the mappings   G & F from contradicting each other.\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4CeKriLTj92",
        "colab_type": "text"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbVsdfHZSr-6",
        "colab_type": "code",
        "outputId": "55132860-8890-42be-9599-a237356192c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30E9u8sYeTR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/drive/My Drive/App/CycleGAN/horse2zebra.zip' /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m60OnKJBejCo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b02f695-54d4-4dae-b5d7-ae5eb509e4a7"
      },
      "source": [
        "!unzip -q /content/horse2zebra.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace horse2zebra/trainA/n02381460_6223.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXduXmJXTnOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cp '/content/drive/My Drive/App/CycleGAN/trainA.64.zip' /content\n",
        "#!cp '/content/drive/My Drive/App/CycleGAN/trainB.64.zip' /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q83ng16CHs-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip -q /content/trainA.64.zip /content\n",
        "#!unzip -q /content/trainB.64.zip /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_To4MyD0IjAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!mv /content/content/trainA.64.npy /content\n",
        "#!mv /content/content/trainB.64.npy /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WBZ4aCkZZYW",
        "colab_type": "text"
      },
      "source": [
        "## Import Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKW2ZgdlYUZB",
        "colab_type": "code",
        "outputId": "d46cb3cb-0d39-4e92-b641-a35d0f5e1302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-89sov9h0\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-89sov9h0\n",
            "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.16.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101066 sha256=30295ea1a7a071c004cf95bf71d2b598863a39db28f65af8bb1c3e1fa7814d13\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pqweu7i5/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeCFZUcmtQU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "62ac52ba-b051-40c0-856f-22ac229d3b14"
      },
      "source": [
        "!pip install pillow"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDc4gom7V_2u",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFd0MepDWBUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import scipy\n",
        "#import scipy.misc\n",
        "#import imageio\n",
        "import cv2\n",
        "from glob import glob\n",
        "import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdXkYCvpVkhL",
        "colab_type": "text"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti1mYLkGVmVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TRAIN_A_DATA_FILE = '/content/trainA.64.npy'\n",
        "#TRAIN_B_DATA_FILE = '/content/trainB.64.npy'\n",
        "\n",
        "DATA_LOC = '/content/horse2zebra'\n",
        "TRAIN_A_LOC = os.path.join(DATA_LOC, 'trainA') \n",
        "TRAIN_B_LOC = os.path.join(DATA_LOC, 'trainB') \n",
        "TEST_A_LOC  = os.path.join(DATA_LOC, 'testA') \n",
        "TEST_B_LOC  = os.path.join(DATA_LOC, 'testB') \n",
        "\n",
        "# image details\n",
        "img_rows = 64\n",
        "img_cols = 64\n",
        "channels = 3\n",
        "image_shape = (img_rows, img_cols, channels)\n",
        "img_res = 64\n",
        "\n",
        "## Latent Space (vectors) dimension\n",
        "latent_dim = 100\n",
        "\n",
        "# Discriminator - PatchGAN, the filters doubles at every stage\n",
        "# 64, 128, 256 and 512\n",
        "dis_num_start_filters = 64\n",
        "\n",
        "# calculate the output shape of Discriminator (PatchGAN)\n",
        "patch_size = int(img_rows / 2**4)\n",
        "disc_patch = (patch_size, patch_size, 1)\n",
        "\n",
        "# Generator\n",
        "gen_num_start_filters = 32\n",
        "\n",
        "# Loss weights\n",
        "lambda_cycle = 10.0                    # Cycle-consistency loss\n",
        "lambda_ident = 0.1 * lambda_cycle      # Identity loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEwkAwnqWsFq",
        "colab_type": "text"
      },
      "source": [
        "## Get the Data and Normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns4gTKonsacM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "?cv2.imread"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9UYyuEXrHZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_read(path):\n",
        "    #return scipy.misc.imread(path, mode='RGB').astype(np.float)\n",
        "    #return imageio.imread(path).astype(np.float)\n",
        "    # this will be bgr image.\n",
        "    return cv2.imread(path).astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs5pW1TBePxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load images \n",
        "# output: batch of images\n",
        "def load_images(domain, batch_size=1, is_testing=False):\n",
        "  # trainA, trainB, testA, testB\n",
        "  data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n",
        "  \n",
        "  path = glob('%s/%s/*' % (DATA_LOC, data_type))\n",
        "\n",
        "  batch_images = np.random.choice(path, size=batch_size)\n",
        "\n",
        "  imgs = []\n",
        "  for img_path in batch_images:\n",
        "      img = image_read(img_path)\n",
        "      if not is_testing:\n",
        "          img = cv2.resize(img, (img_res,img_res))\n",
        "\n",
        "          if np.random.random() > 0.5:\n",
        "              img = np.fliplr(img)\n",
        "      else:\n",
        "          img = cv2.resize(img, (img_res, img_res))\n",
        "      imgs.append(img)\n",
        "\n",
        "  imgs = np.array(imgs)/127.5 - 1.\n",
        "\n",
        "  return imgs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF4bvZHhVG50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# like generator, keep supply the batches of images\n",
        "def generate_batch_images(batch_size=1, is_testing=False):\n",
        "  # get the domain\n",
        "  data_type = \"train\" if not is_testing else \"test\"\n",
        "  \n",
        "  path_A = glob('%s/%sA/*' % (DATA_LOC, data_type))\n",
        "  path_B = glob('%s/%sB/*' % (DATA_LOC, data_type))\n",
        "\n",
        "  n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
        "  total_samples = n_batches * batch_size\n",
        "\n",
        "  # Sample n_batches * batch_size from each path list so that model sees all\n",
        "  # samples from both domains\n",
        "  path_A = np.random.choice(path_A, total_samples, replace=False)\n",
        "  path_B = np.random.choice(path_B, total_samples, replace=False)\n",
        "\n",
        "  for i in range(n_batches-1):\n",
        "      batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
        "      batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
        "      imgs_A, imgs_B = [], []\n",
        "      for img_A, img_B in zip(batch_A, batch_B):\n",
        "          img_A = image_read(img_A)\n",
        "          img_B = image_read(img_B)\n",
        "\n",
        "          img_A = cv2.resize(img_A, (img_res,img_res))\n",
        "          img_B = cv2.resize(img_B, (img_res,img_res))\n",
        "\n",
        "          if not is_testing and np.random.random() > 0.5:\n",
        "                  img_A = np.fliplr(img_A)\n",
        "                  img_B = np.fliplr(img_B)\n",
        "\n",
        "          imgs_A.append(img_A)\n",
        "          imgs_B.append(img_B)\n",
        "\n",
        "      imgs_A = np.array(imgs_A)/127.5 - 1.\n",
        "      imgs_B = np.array(imgs_B)/127.5 - 1.\n",
        "\n",
        "      yield imgs_A, imgs_B "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV99bB3mT3KI",
        "colab_type": "text"
      },
      "source": [
        "## Build CycleGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTPnobltT6w_",
        "colab_type": "code",
        "outputId": "40589e27-6e74-4a70-c517-67f1daf62863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# keras layers\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "\n",
        "# from keras_contrib\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0o__psTdewx",
        "colab_type": "text"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pDF6KO-diVx",
        "colab_type": "text"
      },
      "source": [
        "For Discriminator:\n",
        "*  Use PatchGAN - only penalizes the structure at the scale of patches.\n",
        "* PatchGAN classifies the NxN patch is real or fake\n",
        "*  They have fewer parameters than the full image discriminator\n",
        "* PatchGAN are used in [Image to Image translation](https://arxiv.org/pdf/1611.07004.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GF5FuQcXxuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator layer has the following\n",
        "#  * Conv2D - filter size: 4x4, strides:2\n",
        "#  * LeakyReLU\n",
        "#  * InstanceNormalization\n",
        "#\n",
        "def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
        "  d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  if normalization:\n",
        "      d = InstanceNormalization()(d)\n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YXsiOGIgLDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build discriminator uses PatchGAN\n",
        "# Uses the patch to classify the image is fake or real.\n",
        "# PatchGAN uses \n",
        "#  * kernel size 4x4\n",
        "#  * num filters double at each stage\n",
        "def build_discriminator():\n",
        "  img = Input(image_shape)\n",
        "  \n",
        "  d1 = d_layer(img, dis_num_start_filters, normalization=False)\n",
        "  d2 = d_layer(d1, dis_num_start_filters*2)\n",
        "  d3 = d_layer(d2, dis_num_start_filters*4)\n",
        "  d4 = d_layer(d3, dis_num_start_filters*8)\n",
        "\n",
        "  validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
        "\n",
        "  return Model(img, validity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zto-D-lKiTMX",
        "colab_type": "text"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OFkkd9Dich1",
        "colab_type": "text"
      },
      "source": [
        "Generator can be one of the following two things:\n",
        "\n",
        "     * Encoder : Decoder combo\n",
        "     or\n",
        "     * Encoder : Transformer : Decoder\n",
        "     \n",
        " The Encoder shrinks the input image. Uses Conv layers (with strides:2).\n",
        " \n",
        " The Transformer uses residual blocks\n",
        " \n",
        " The Decoder expands the image with transpose Conv.\n",
        " \n",
        " Note: each layer will use LeakyReLU and InstanceNormalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmP-lGsWiUd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define encoder layer\n",
        "\n",
        "def encoder_layer(layer_input, filters, f_size=4):\n",
        "  # Downsamples the input \n",
        "  d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  d = InstanceNormalization()(d)\n",
        "  return d\n",
        "\n",
        "# define decoder layer\n",
        "def decoder_layer(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "  # upsample the input\n",
        "  u = UpSampling2D(size=2)(layer_input)\n",
        "  u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
        "  if dropout_rate:\n",
        "      u = Dropout(dropout_rate)(u)\n",
        "  u = InstanceNormalization()(u)\n",
        "  u = Concatenate()([u, skip_input])\n",
        "  return u"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0fEf2oNn2xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the generator (image -> image)\n",
        "#  \n",
        "def build_generator():\n",
        "  # Image input\n",
        "  d0 = Input(shape=image_shape)\n",
        "\n",
        "  # Downsample\n",
        "  d1 = encoder_layer(d0, gen_num_start_filters)\n",
        "  d2 = encoder_layer(d1, gen_num_start_filters*2)\n",
        "  d3 = encoder_layer(d2, gen_num_start_filters*4)\n",
        "  d4 = encoder_layer(d3, gen_num_start_filters*8)\n",
        "\n",
        "  # Upsample\n",
        "  u1 = decoder_layer(d4, d3, gen_num_start_filters*4)\n",
        "  u2 = decoder_layer(u1, d2, gen_num_start_filters*2)\n",
        "  u3 = decoder_layer(u2, d1, gen_num_start_filters)\n",
        "\n",
        "  u4 = UpSampling2D(size=2)(u3)\n",
        "  output_img = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
        "\n",
        "  return Model(d0, output_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-yCZMyHpdBI",
        "colab_type": "text"
      },
      "source": [
        "## Build CycleGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TJVQrubperu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = Adam(0.0002, 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFTTUC4YuFwe",
        "colab_type": "text"
      },
      "source": [
        "###  Discriminators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOkZZJBipqsN",
        "colab_type": "code",
        "outputId": "cda49fd9-9d48-494b-beaf-2fcef9da1724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# build and compile discriminators\n",
        "#\n",
        "dis_A = build_discriminator()\n",
        "dis_B = build_discriminator()\n",
        "\n",
        "dis_A.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
        "dis_B.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0804 10:06:17.310028 140151751292800 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0804 10:06:17.313156 140151751292800 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0804 10:06:17.319377 140151751292800 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0804 10:06:17.633653 140151751292800 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISym7_-4t-1s",
        "colab_type": "text"
      },
      "source": [
        "###   Generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huWngURtuCTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6238ead3-c090-45d1-bffa-b3b16a616e16"
      },
      "source": [
        "# build and compile generators\n",
        "gen_AB = build_generator()\n",
        "gen_BA = build_generator()\n",
        "\n",
        "# input images for both domains\n",
        "image_A = Input(shape=image_shape)\n",
        "\n",
        "# translate the image to other domain\n",
        "fake_B  = gen_AB(image_A)\n",
        "\n",
        "# reconstructed image\n",
        "reconstr_A = gen_BA(fake_B)\n",
        "\n",
        "# other image as well.\n",
        "image_B = Input(shape=image_shape)\n",
        "fake_A  = gen_BA(image_B)\n",
        "reconstr_B = gen_AB(fake_A)\n",
        "\n",
        "# identify mappings for both images\n",
        "ident_A = gen_BA(image_A)\n",
        "ident_B = gen_AB(image_B)\n",
        "\n",
        "# for the combined model, train only generators.\n",
        "# so, freeze the discriminators\n",
        "dis_A.trainable = False\n",
        "dis_B.trainable = False\n",
        "\n",
        "# discriminators detemine the authenticity of the translated images\n",
        "result_A = dis_A(fake_A)\n",
        "result_B = dis_B(fake_B)\n",
        "\n",
        "# combined model\n",
        "combined = Model(inputs = [image_A, image_B],\n",
        "                 outputs = [result_A, result_B, \n",
        "                            reconstr_A, reconstr_B, ident_A, ident_B])\n",
        "\n",
        "# compile\n",
        "combined.compile(loss=['mse', 'mse', \n",
        "                       'mae', 'mae',\n",
        "                       'mae', 'mae'],\n",
        "                 loss_weights=[  1, 1,\n",
        "                                 lambda_cycle, lambda_cycle,\n",
        "                                 lambda_ident, lambda_ident ],\n",
        "                  optimizer=optimizer)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0804 10:06:17.814677 140151751292800 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTQJKYCNyHGE",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM5TTQxOGQO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK46mbozagY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample images from test suite to see the progress.\n",
        "def sample_images(epoch, batch_idx):\n",
        "  os.makedirs('/content/gen_images', exist_ok=True)\n",
        "  r, c = 2, 3\n",
        "\n",
        "  imgs_A = load_images(domain=\"A\", batch_size=1, is_testing=True)\n",
        "  imgs_B = load_images(domain=\"B\", batch_size=1, is_testing=True)\n",
        "\n",
        "  # Demo (for GIF)\n",
        "  #imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n",
        "  #imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n",
        "\n",
        "  # Translate images to the other domain\n",
        "  fake_B = gen_AB.predict(imgs_A)\n",
        "  fake_A = gen_BA.predict(imgs_B)\n",
        "  \n",
        "  # Translate back to original domain\n",
        "  reconstr_A = gen_BA.predict(fake_B)\n",
        "  reconstr_B = gen_AB.predict(fake_A)\n",
        "\n",
        "  gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, \n",
        "                             imgs_B, fake_A, reconstr_B])\n",
        "\n",
        "  # Rescale images 0 - 1\n",
        "  gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "  titles = ['Original', 'Translated', 'Reconstructed']\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  cnt = 0\n",
        "  for i in range(r):\n",
        "      for j in range(c):\n",
        "          axs[i,j].imshow(gen_imgs[cnt])\n",
        "          axs[i, j].set_title(titles[j])\n",
        "          axs[i,j].axis('off')\n",
        "          cnt += 1\n",
        "  fig.savefig(\"/content/gen_images/%d_%d.png\" % (epoch, batch_idx))\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpjexTHJJq2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf /content/gen_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBY_kTl3yIfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 50\n",
        "batch_size = 1\n",
        "sample_interval = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCOmLyi59Cu9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf602a0f-f2d7-4339-c786-d9f425ec618e"
      },
      "source": [
        "# Adversarial loss ground truths\n",
        "valid = np.ones((batch_size,) + disc_patch)\n",
        "fake = np.zeros((batch_size,) + disc_patch)\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  \n",
        "  \n",
        "  #process in batches\n",
        "  for batch_i, (imgs_A, imgs_B) in enumerate(generate_batch_images(batch_size)):\n",
        "  \n",
        "    # ----------------------\n",
        "    #  Train Discriminators\n",
        "    # ----------------------\n",
        "\n",
        "    # Translate images to opposite domain\n",
        "    fake_B = gen_AB.predict(imgs_A)\n",
        "    fake_A = gen_BA.predict(imgs_B)\n",
        "\n",
        "    # Train the discriminators (original images = real / translated = Fake)\n",
        "    dA_loss_real = dis_A.train_on_batch(imgs_A, valid)\n",
        "    dA_loss_fake = dis_A.train_on_batch(fake_A, fake)\n",
        "    dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
        "\n",
        "    dB_loss_real = dis_B.train_on_batch(imgs_B, valid)\n",
        "    dB_loss_fake = dis_B.train_on_batch(fake_B, fake)\n",
        "    dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
        "\n",
        "    # Total disciminator loss\n",
        "    d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
        "\n",
        "    # ------------------\n",
        "    #  Train Generators\n",
        "    # ------------------\n",
        "\n",
        "    # Train the generators\n",
        "    g_loss = combined.train_on_batch([imgs_A, imgs_B],\n",
        "                                     [valid, valid, \n",
        "                                      imgs_A, imgs_B,\n",
        "                                      imgs_A, imgs_B]) \n",
        "    \n",
        "    elapsed_time = datetime.datetime.now() - start_time\n",
        "    \n",
        "    if batch_i % sample_interval == 0:\n",
        "      print (\"[Epoch %d/%d] [Batch %d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
        "                                                                        % ( epoch, epochs,\n",
        "                                                                            batch_i,  \n",
        "                                                                            d_loss[0], 100*d_loss[1],\n",
        "                                                                            g_loss[0],\n",
        "                                                                            np.mean(g_loss[1:3]),\n",
        "                                                                            np.mean(g_loss[3:5]),\n",
        "                                                                            np.mean(g_loss[5:6]),\n",
        "                                                                            elapsed_time))\n",
        "      sample_images(epoch, batch_i)\n",
        "   "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0/50] [Batch 0] [D loss: 0.344585, acc:  45%] [G loss: 9.771956, adv: 0.540866, recon: 0.389615, id: 0.449109] time: 0:00:00.164245 \n",
            "[Epoch 0/50] [Batch 500] [D loss: 0.273496, acc:  43%] [G loss: 5.784718, adv: 0.477119, recon: 0.212756, id: 0.302350] time: 0:01:02.058005 \n",
            "[Epoch 0/50] [Batch 1000] [D loss: 0.251443, acc:  54%] [G loss: 4.000812, adv: 0.494759, recon: 0.129137, id: 0.183176] time: 0:02:03.212105 \n",
            "[Epoch 1/50] [Batch 0] [D loss: 0.185995, acc:  64%] [G loss: 7.728089, adv: 0.557735, recon: 0.297731, id: 0.281763] time: 0:02:11.517422 \n",
            "[Epoch 1/50] [Batch 500] [D loss: 0.427785, acc:  26%] [G loss: 5.238328, adv: 0.711084, recon: 0.170138, id: 0.241952] time: 0:03:13.073731 \n",
            "[Epoch 1/50] [Batch 1000] [D loss: 0.169545, acc:  75%] [G loss: 4.297024, adv: 0.721930, recon: 0.125548, id: 0.142691] time: 0:04:14.319814 \n",
            "[Epoch 2/50] [Batch 0] [D loss: 0.141728, acc:  81%] [G loss: 4.470566, adv: 0.687624, recon: 0.136154, id: 0.144231] time: 0:04:22.445932 \n",
            "[Epoch 2/50] [Batch 500] [D loss: 0.153569, acc:  73%] [G loss: 6.034175, adv: 0.666002, recon: 0.207156, id: 0.179779] time: 0:05:23.326506 \n",
            "[Epoch 2/50] [Batch 1000] [D loss: 0.320370, acc:  59%] [G loss: 5.679558, adv: 0.741771, recon: 0.185510, id: 0.215559] time: 0:06:23.745187 \n",
            "[Epoch 3/50] [Batch 0] [D loss: 0.056675, acc:  92%] [G loss: 7.155388, adv: 0.898280, recon: 0.234833, id: 0.266184] time: 0:06:31.829779 \n",
            "[Epoch 3/50] [Batch 500] [D loss: 0.081479, acc:  95%] [G loss: 5.023728, adv: 0.988843, recon: 0.131953, id: 0.247101] time: 0:07:32.461775 \n",
            "[Epoch 3/50] [Batch 1000] [D loss: 0.045210, acc:  95%] [G loss: 6.793349, adv: 0.971273, recon: 0.212834, id: 0.220849] time: 0:08:32.585754 \n",
            "[Epoch 4/50] [Batch 0] [D loss: 0.038434, acc: 100%] [G loss: 4.741159, adv: 0.967950, recon: 0.125408, id: 0.153269] time: 0:08:40.784846 \n",
            "[Epoch 4/50] [Batch 500] [D loss: 0.063622, acc:  90%] [G loss: 4.686269, adv: 0.753403, recon: 0.140753, id: 0.185217] time: 0:09:41.385332 \n",
            "[Epoch 4/50] [Batch 1000] [D loss: 0.062574, acc:  90%] [G loss: 5.472766, adv: 1.176785, recon: 0.132317, id: 0.316085] time: 0:10:42.138709 \n",
            "[Epoch 5/50] [Batch 0] [D loss: 0.425035, acc:  46%] [G loss: 4.981506, adv: 0.870997, recon: 0.143374, id: 0.243956] time: 0:10:50.256865 \n",
            "[Epoch 5/50] [Batch 500] [D loss: 0.095220, acc:  81%] [G loss: 5.618639, adv: 0.875072, recon: 0.166411, id: 0.176854] time: 0:11:50.611334 \n",
            "[Epoch 5/50] [Batch 1000] [D loss: 0.090385, acc:  81%] [G loss: 4.786777, adv: 0.938657, recon: 0.129673, id: 0.167718] time: 0:12:50.664005 \n",
            "[Epoch 6/50] [Batch 0] [D loss: 0.029461, acc: 100%] [G loss: 4.291806, adv: 0.826642, recon: 0.112036, id: 0.102163] time: 0:12:58.718944 \n",
            "[Epoch 6/50] [Batch 500] [D loss: 0.047428, acc: 100%] [G loss: 5.435043, adv: 0.983563, recon: 0.154100, id: 0.163393] time: 0:13:58.740959 \n",
            "[Epoch 6/50] [Batch 1000] [D loss: 0.015390, acc: 100%] [G loss: 4.840469, adv: 0.952799, recon: 0.130677, id: 0.151439] time: 0:14:58.895931 \n",
            "[Epoch 7/50] [Batch 0] [D loss: 0.203902, acc:  75%] [G loss: 5.994820, adv: 1.042461, recon: 0.172710, id: 0.181975] time: 0:15:06.978805 \n",
            "[Epoch 7/50] [Batch 500] [D loss: 0.072188, acc:  93%] [G loss: 4.357885, adv: 1.001238, recon: 0.092557, id: 0.378030] time: 0:16:06.890667 \n",
            "[Epoch 7/50] [Batch 1000] [D loss: 0.068930, acc:  90%] [G loss: 4.126774, adv: 0.931471, recon: 0.096839, id: 0.202672] time: 0:17:06.875305 \n",
            "[Epoch 8/50] [Batch 0] [D loss: 0.081763, acc:  79%] [G loss: 4.868055, adv: 0.885406, recon: 0.133999, id: 0.192808] time: 0:17:14.953131 \n",
            "[Epoch 8/50] [Batch 500] [D loss: 0.010662, acc: 100%] [G loss: 4.675585, adv: 1.048338, recon: 0.112250, id: 0.147685] time: 0:18:14.907083 \n",
            "[Epoch 8/50] [Batch 1000] [D loss: 0.024451, acc: 100%] [G loss: 5.376002, adv: 0.941904, recon: 0.145015, id: 0.441706] time: 0:19:15.065816 \n",
            "[Epoch 9/50] [Batch 0] [D loss: 0.366666, acc:  26%] [G loss: 5.151343, adv: 0.968209, recon: 0.143060, id: 0.180465] time: 0:19:23.071271 \n",
            "[Epoch 9/50] [Batch 500] [D loss: 0.056915, acc:  95%] [G loss: 6.468274, adv: 1.090448, recon: 0.189446, id: 0.165416] time: 0:20:23.211206 \n",
            "[Epoch 9/50] [Batch 1000] [D loss: 0.072732, acc:  96%] [G loss: 4.525913, adv: 0.756950, recon: 0.131997, id: 0.144153] time: 0:21:23.595926 \n",
            "[Epoch 10/50] [Batch 0] [D loss: 0.154745, acc:  75%] [G loss: 4.013738, adv: 0.773659, recon: 0.106653, id: 0.152817] time: 0:21:31.679273 \n",
            "[Epoch 10/50] [Batch 500] [D loss: 0.031717, acc: 100%] [G loss: 4.823508, adv: 0.860086, recon: 0.128002, id: 0.306584] time: 0:22:32.193759 \n",
            "[Epoch 10/50] [Batch 1000] [D loss: 0.027800, acc: 100%] [G loss: 4.067292, adv: 0.875268, recon: 0.101316, id: 0.112261] time: 0:23:32.422700 \n",
            "[Epoch 11/50] [Batch 0] [D loss: 0.063171, acc:  92%] [G loss: 5.068618, adv: 0.940176, recon: 0.142771, id: 0.155640] time: 0:23:40.451501 \n",
            "[Epoch 11/50] [Batch 500] [D loss: 0.111785, acc:  87%] [G loss: 4.581617, adv: 0.936160, recon: 0.116071, id: 0.199164] time: 0:24:40.864812 \n",
            "[Epoch 11/50] [Batch 1000] [D loss: 0.129878, acc:  84%] [G loss: 3.848379, adv: 0.702319, recon: 0.106902, id: 0.104069] time: 0:25:41.194124 \n",
            "[Epoch 12/50] [Batch 0] [D loss: 0.017699, acc: 100%] [G loss: 5.336825, adv: 1.096597, recon: 0.140350, id: 0.187099] time: 0:25:49.216072 \n",
            "[Epoch 12/50] [Batch 500] [D loss: 0.089103, acc:  81%] [G loss: 4.833810, adv: 0.862929, recon: 0.137231, id: 0.196839] time: 0:26:48.781038 \n",
            "[Epoch 12/50] [Batch 1000] [D loss: 0.090100, acc:  87%] [G loss: 5.252135, adv: 1.016706, recon: 0.141259, id: 0.251076] time: 0:27:48.612025 \n",
            "[Epoch 13/50] [Batch 0] [D loss: 0.108527, acc:  84%] [G loss: 5.668964, adv: 0.999522, recon: 0.163175, id: 0.146313] time: 0:27:56.604683 \n",
            "[Epoch 13/50] [Batch 500] [D loss: 0.007882, acc: 100%] [G loss: 4.788604, adv: 1.022034, recon: 0.113554, id: 0.278446] time: 0:28:57.038993 \n",
            "[Epoch 13/50] [Batch 1000] [D loss: 0.015982, acc: 100%] [G loss: 5.398179, adv: 1.120788, recon: 0.138360, id: 0.194027] time: 0:29:57.444281 \n",
            "[Epoch 14/50] [Batch 0] [D loss: 0.098007, acc:  82%] [G loss: 4.283696, adv: 0.820164, recon: 0.116175, id: 0.148202] time: 0:30:05.913291 \n",
            "[Epoch 14/50] [Batch 500] [D loss: 0.029151, acc: 100%] [G loss: 4.713082, adv: 0.987625, recon: 0.119172, id: 0.211970] time: 0:31:06.673874 \n",
            "[Epoch 14/50] [Batch 1000] [D loss: 0.047560, acc: 100%] [G loss: 4.995146, adv: 0.862595, recon: 0.143761, id: 0.196723] time: 0:32:06.353495 \n",
            "[Epoch 15/50] [Batch 0] [D loss: 0.072354, acc:  98%] [G loss: 4.407296, adv: 0.862485, recon: 0.118637, id: 0.140774] time: 0:32:14.230685 \n",
            "[Epoch 15/50] [Batch 500] [D loss: 0.008917, acc: 100%] [G loss: 3.973567, adv: 0.872962, recon: 0.093828, id: 0.172873] time: 0:33:14.055579 \n",
            "[Epoch 15/50] [Batch 1000] [D loss: 0.059809, acc:  98%] [G loss: 3.614416, adv: 0.846805, recon: 0.082705, id: 0.115680] time: 0:34:14.247627 \n",
            "[Epoch 16/50] [Batch 0] [D loss: 0.117238, acc:  95%] [G loss: 4.126761, adv: 0.899310, recon: 0.099636, id: 0.207504] time: 0:34:22.326208 \n",
            "[Epoch 16/50] [Batch 500] [D loss: 0.008421, acc: 100%] [G loss: 4.157281, adv: 0.987859, recon: 0.096593, id: 0.107662] time: 0:35:22.310966 \n",
            "[Epoch 16/50] [Batch 1000] [D loss: 0.074280, acc:  89%] [G loss: 4.648026, adv: 0.913025, recon: 0.122258, id: 0.180659] time: 0:36:22.233130 \n",
            "[Epoch 17/50] [Batch 0] [D loss: 0.014936, acc: 100%] [G loss: 4.903038, adv: 0.960741, recon: 0.128745, id: 0.189242] time: 0:36:30.144730 \n",
            "[Epoch 17/50] [Batch 500] [D loss: 0.037876, acc: 100%] [G loss: 4.417889, adv: 0.928525, recon: 0.109324, id: 0.158566] time: 0:37:30.046847 \n",
            "[Epoch 17/50] [Batch 1000] [D loss: 0.014856, acc: 100%] [G loss: 4.002481, adv: 0.964349, recon: 0.090304, id: 0.128676] time: 0:38:29.663133 \n",
            "[Epoch 18/50] [Batch 0] [D loss: 0.014772, acc: 100%] [G loss: 5.059203, adv: 1.019126, recon: 0.129400, id: 0.112704] time: 0:38:37.636009 \n",
            "[Epoch 18/50] [Batch 500] [D loss: 0.182112, acc:  67%] [G loss: 4.163127, adv: 0.910674, recon: 0.098755, id: 0.148843] time: 0:39:37.743805 \n",
            "[Epoch 18/50] [Batch 1000] [D loss: 0.065195, acc:  92%] [G loss: 4.213697, adv: 0.980800, recon: 0.099317, id: 0.120502] time: 0:40:37.861667 \n",
            "[Epoch 19/50] [Batch 0] [D loss: 0.024757, acc: 100%] [G loss: 4.709019, adv: 1.063339, recon: 0.113386, id: 0.127271] time: 0:40:45.948469 \n",
            "[Epoch 19/50] [Batch 500] [D loss: 0.015986, acc: 100%] [G loss: 4.437091, adv: 0.998972, recon: 0.101838, id: 0.207180] time: 0:41:46.177033 \n",
            "[Epoch 19/50] [Batch 1000] [D loss: 0.047341, acc:  98%] [G loss: 4.044770, adv: 0.772896, recon: 0.108509, id: 0.113116] time: 0:42:45.815626 \n",
            "[Epoch 20/50] [Batch 0] [D loss: 0.017533, acc: 100%] [G loss: 4.400061, adv: 1.034210, recon: 0.101635, id: 0.127262] time: 0:42:53.736836 \n",
            "[Epoch 20/50] [Batch 500] [D loss: 0.029116, acc: 100%] [G loss: 4.610035, adv: 1.057497, recon: 0.109886, id: 0.107751] time: 0:43:53.294365 \n",
            "[Epoch 20/50] [Batch 1000] [D loss: 0.022892, acc: 100%] [G loss: 4.453370, adv: 0.969224, recon: 0.107608, id: 0.205956] time: 0:44:53.622666 \n",
            "[Epoch 21/50] [Batch 0] [D loss: 0.040790, acc:  90%] [G loss: 4.055750, adv: 0.914990, recon: 0.094232, id: 0.176430] time: 0:45:01.834591 \n",
            "[Epoch 21/50] [Batch 500] [D loss: 0.092601, acc:  78%] [G loss: 4.484727, adv: 1.025506, recon: 0.098833, id: 0.291215] time: 0:46:02.763314 \n",
            "[Epoch 21/50] [Batch 1000] [D loss: 0.079783, acc:  96%] [G loss: 3.905366, adv: 0.942369, recon: 0.085716, id: 0.179298] time: 0:47:03.453707 \n",
            "[Epoch 22/50] [Batch 0] [D loss: 0.092163, acc:  92%] [G loss: 4.134399, adv: 1.062486, recon: 0.084081, id: 0.208703] time: 0:47:11.618628 \n",
            "[Epoch 22/50] [Batch 500] [D loss: 0.070298, acc:  92%] [G loss: 3.918239, adv: 0.829927, recon: 0.092882, id: 0.129693] time: 0:48:12.992692 \n",
            "[Epoch 22/50] [Batch 1000] [D loss: 0.034809, acc: 100%] [G loss: 4.632039, adv: 1.249650, recon: 0.085821, id: 0.277700] time: 0:49:14.322766 \n",
            "[Epoch 23/50] [Batch 0] [D loss: 0.007991, acc: 100%] [G loss: 4.143071, adv: 0.981455, recon: 0.093229, id: 0.108866] time: 0:49:22.553151 \n",
            "[Epoch 23/50] [Batch 500] [D loss: 0.040011, acc:  98%] [G loss: 4.852194, adv: 1.215873, recon: 0.106139, id: 0.156468] time: 0:50:23.321795 \n",
            "[Epoch 23/50] [Batch 1000] [D loss: 0.040671, acc: 100%] [G loss: 4.133405, adv: 0.950435, recon: 0.094352, id: 0.132252] time: 0:51:24.379865 \n",
            "[Epoch 24/50] [Batch 0] [D loss: 0.010029, acc: 100%] [G loss: 4.282119, adv: 1.045427, recon: 0.093071, id: 0.197222] time: 0:51:32.434440 \n",
            "[Epoch 24/50] [Batch 500] [D loss: 0.042403, acc: 100%] [G loss: 4.581346, adv: 0.956311, recon: 0.118035, id: 0.145843] time: 0:52:33.001976 \n",
            "[Epoch 24/50] [Batch 1000] [D loss: 0.009855, acc: 100%] [G loss: 4.052223, adv: 0.969329, recon: 0.089822, id: 0.127602] time: 0:53:33.438804 \n",
            "[Epoch 25/50] [Batch 0] [D loss: 0.007392, acc: 100%] [G loss: 5.293540, adv: 0.952167, recon: 0.145900, id: 0.142769] time: 0:53:41.471485 \n",
            "[Epoch 25/50] [Batch 500] [D loss: 0.013276, acc: 100%] [G loss: 4.676015, adv: 1.042261, recon: 0.109735, id: 0.198236] time: 0:54:42.263953 \n",
            "[Epoch 25/50] [Batch 1000] [D loss: 0.030755, acc:  98%] [G loss: 4.129422, adv: 0.916242, recon: 0.098630, id: 0.116918] time: 0:55:42.846563 \n",
            "[Epoch 26/50] [Batch 0] [D loss: 0.022441, acc: 100%] [G loss: 3.885794, adv: 0.872613, recon: 0.090970, id: 0.130734] time: 0:55:50.943091 \n",
            "[Epoch 26/50] [Batch 500] [D loss: 0.013211, acc: 100%] [G loss: 3.729690, adv: 0.984289, recon: 0.074359, id: 0.170773] time: 0:56:52.071125 \n",
            "[Epoch 26/50] [Batch 1000] [D loss: 0.072052, acc:  84%] [G loss: 4.442966, adv: 1.046861, recon: 0.096707, id: 0.272955] time: 0:57:52.686363 \n",
            "[Epoch 27/50] [Batch 0] [D loss: 0.004163, acc: 100%] [G loss: 4.444301, adv: 1.007776, recon: 0.105597, id: 0.146380] time: 0:58:00.776281 \n",
            "[Epoch 27/50] [Batch 500] [D loss: 0.018998, acc: 100%] [G loss: 4.299241, adv: 0.982329, recon: 0.095651, id: 0.240223] time: 0:59:01.482287 \n",
            "[Epoch 27/50] [Batch 1000] [D loss: 0.061183, acc: 100%] [G loss: 3.580750, adv: 0.908319, recon: 0.074616, id: 0.110237] time: 1:00:01.527264 \n",
            "[Epoch 28/50] [Batch 0] [D loss: 0.090148, acc:  87%] [G loss: 3.540110, adv: 0.946616, recon: 0.071557, id: 0.091401] time: 1:00:09.484019 \n",
            "[Epoch 28/50] [Batch 500] [D loss: 0.002814, acc: 100%] [G loss: 3.669858, adv: 1.020395, recon: 0.062077, id: 0.303207] time: 1:01:09.916690 \n",
            "[Epoch 28/50] [Batch 1000] [D loss: 0.110183, acc:  93%] [G loss: 3.966763, adv: 1.113926, recon: 0.069216, id: 0.133090] time: 1:02:09.965391 \n",
            "[Epoch 29/50] [Batch 0] [D loss: 0.006326, acc: 100%] [G loss: 4.478650, adv: 1.000321, recon: 0.104871, id: 0.201016] time: 1:02:17.980367 \n",
            "[Epoch 29/50] [Batch 500] [D loss: 0.018397, acc: 100%] [G loss: 3.513773, adv: 0.915315, recon: 0.068810, id: 0.166484] time: 1:03:18.077256 \n",
            "[Epoch 29/50] [Batch 1000] [D loss: 0.006047, acc: 100%] [G loss: 4.759787, adv: 0.995783, recon: 0.122009, id: 0.161792] time: 1:04:18.115552 \n",
            "[Epoch 30/50] [Batch 0] [D loss: 0.030275, acc: 100%] [G loss: 4.553952, adv: 1.151538, recon: 0.095671, id: 0.176139] time: 1:04:26.125204 \n",
            "[Epoch 30/50] [Batch 500] [D loss: 0.006796, acc: 100%] [G loss: 4.446203, adv: 0.992507, recon: 0.105606, id: 0.152686] time: 1:05:25.981963 \n",
            "[Epoch 30/50] [Batch 1000] [D loss: 0.089737, acc:  79%] [G loss: 3.298121, adv: 0.886041, recon: 0.058670, id: 0.184593] time: 1:06:25.665393 \n",
            "[Epoch 31/50] [Batch 0] [D loss: 0.015962, acc: 100%] [G loss: 4.128591, adv: 1.032640, recon: 0.088069, id: 0.174439] time: 1:06:33.735960 \n",
            "[Epoch 31/50] [Batch 500] [D loss: 0.009926, acc: 100%] [G loss: 3.502403, adv: 0.994411, recon: 0.061017, id: 0.192854] time: 1:07:33.428214 \n",
            "[Epoch 31/50] [Batch 1000] [D loss: 0.008349, acc: 100%] [G loss: 4.129789, adv: 0.921142, recon: 0.094974, id: 0.206179] time: 1:08:33.105709 \n",
            "[Epoch 32/50] [Batch 0] [D loss: 0.007239, acc: 100%] [G loss: 4.356184, adv: 0.997456, recon: 0.099661, id: 0.191662] time: 1:08:41.068393 \n",
            "[Epoch 32/50] [Batch 500] [D loss: 0.013740, acc: 100%] [G loss: 3.593475, adv: 1.007386, recon: 0.070900, id: 0.058164] time: 1:09:41.757645 \n",
            "[Epoch 32/50] [Batch 1000] [D loss: 0.007705, acc: 100%] [G loss: 4.784059, adv: 0.977330, recon: 0.115766, id: 0.222738] time: 1:10:43.071493 \n",
            "[Epoch 33/50] [Batch 0] [D loss: 0.010376, acc: 100%] [G loss: 4.074840, adv: 0.980285, recon: 0.087849, id: 0.203572] time: 1:10:51.496654 \n",
            "[Epoch 33/50] [Batch 500] [D loss: 0.016086, acc: 100%] [G loss: 4.402470, adv: 1.030724, recon: 0.098777, id: 0.213010] time: 1:11:54.592888 \n",
            "[Epoch 33/50] [Batch 1000] [D loss: 0.050831, acc:  98%] [G loss: 3.560389, adv: 0.906761, recon: 0.074992, id: 0.133724] time: 1:12:55.495691 \n",
            "[Epoch 34/50] [Batch 0] [D loss: 0.008494, acc: 100%] [G loss: 4.429324, adv: 0.995821, recon: 0.099397, id: 0.260186] time: 1:13:03.667711 \n",
            "[Epoch 34/50] [Batch 500] [D loss: 0.019636, acc: 100%] [G loss: 4.241477, adv: 1.046985, recon: 0.089457, id: 0.235447] time: 1:14:04.213058 \n",
            "[Epoch 34/50] [Batch 1000] [D loss: 0.001968, acc: 100%] [G loss: 4.044791, adv: 0.995170, recon: 0.088127, id: 0.120838] time: 1:15:04.403717 \n",
            "[Epoch 35/50] [Batch 0] [D loss: 0.027253, acc: 100%] [G loss: 4.455944, adv: 0.989227, recon: 0.108830, id: 0.127478] time: 1:15:12.421431 \n",
            "[Epoch 35/50] [Batch 500] [D loss: 0.003099, acc: 100%] [G loss: 4.850877, adv: 1.031952, recon: 0.113370, id: 0.220169] time: 1:16:12.441548 \n",
            "[Epoch 35/50] [Batch 1000] [D loss: 0.005049, acc: 100%] [G loss: 3.812431, adv: 1.042864, recon: 0.073421, id: 0.119862] time: 1:17:12.370621 \n",
            "[Epoch 36/50] [Batch 0] [D loss: 0.006927, acc: 100%] [G loss: 3.958215, adv: 1.004069, recon: 0.083915, id: 0.080567] time: 1:17:20.370286 \n",
            "[Epoch 36/50] [Batch 500] [D loss: 0.004979, acc: 100%] [G loss: 4.228293, adv: 1.005705, recon: 0.098029, id: 0.114029] time: 1:18:19.777320 \n",
            "[Epoch 36/50] [Batch 1000] [D loss: 0.003275, acc: 100%] [G loss: 3.807962, adv: 1.004155, recon: 0.080227, id: 0.083268] time: 1:19:19.370158 \n",
            "[Epoch 37/50] [Batch 0] [D loss: 0.117149, acc:  75%] [G loss: 4.275516, adv: 1.018247, recon: 0.101181, id: 0.078951] time: 1:19:27.463310 \n",
            "[Epoch 37/50] [Batch 500] [D loss: 0.013512, acc: 100%] [G loss: 4.124652, adv: 0.910984, recon: 0.089210, id: 0.352092] time: 1:20:26.983975 \n",
            "[Epoch 37/50] [Batch 1000] [D loss: 0.030398, acc: 100%] [G loss: 3.244679, adv: 0.839536, recon: 0.069829, id: 0.090781] time: 1:21:26.361645 \n",
            "[Epoch 38/50] [Batch 0] [D loss: 0.078828, acc:  93%] [G loss: 5.000157, adv: 1.727298, recon: 0.066562, id: 0.107375] time: 1:21:34.297130 \n",
            "[Epoch 38/50] [Batch 500] [D loss: 0.035004, acc: 100%] [G loss: 3.871045, adv: 1.030958, recon: 0.077716, id: 0.148498] time: 1:22:34.033834 \n",
            "[Epoch 38/50] [Batch 1000] [D loss: 0.024263, acc: 100%] [G loss: 3.507183, adv: 0.965199, recon: 0.067945, id: 0.144611] time: 1:23:33.600906 \n",
            "[Epoch 39/50] [Batch 0] [D loss: 0.002750, acc: 100%] [G loss: 4.031757, adv: 1.030932, recon: 0.085112, id: 0.121441] time: 1:23:41.558580 \n",
            "[Epoch 39/50] [Batch 500] [D loss: 0.042509, acc: 100%] [G loss: 4.031319, adv: 0.934416, recon: 0.095579, id: 0.132907] time: 1:24:40.782323 \n",
            "[Epoch 39/50] [Batch 1000] [D loss: 0.022072, acc: 100%] [G loss: 3.619211, adv: 1.043893, recon: 0.065035, id: 0.136167] time: 1:25:40.298761 \n",
            "[Epoch 40/50] [Batch 0] [D loss: 0.014713, acc: 100%] [G loss: 3.599423, adv: 0.952312, recon: 0.074907, id: 0.080205] time: 1:25:48.187556 \n",
            "[Epoch 40/50] [Batch 500] [D loss: 0.032946, acc:  96%] [G loss: 3.532898, adv: 0.979368, recon: 0.069345, id: 0.093589] time: 1:26:47.626140 \n",
            "[Epoch 40/50] [Batch 1000] [D loss: 0.039537, acc: 100%] [G loss: 4.089170, adv: 1.235492, recon: 0.071283, id: 0.093885] time: 1:27:47.542758 \n",
            "[Epoch 41/50] [Batch 0] [D loss: 0.014560, acc: 100%] [G loss: 3.895433, adv: 1.011269, recon: 0.082454, id: 0.109372] time: 1:27:55.505245 \n",
            "[Epoch 41/50] [Batch 500] [D loss: 0.013890, acc: 100%] [G loss: 3.722366, adv: 0.991761, recon: 0.076045, id: 0.105438] time: 1:28:55.347822 \n",
            "[Epoch 41/50] [Batch 1000] [D loss: 0.009316, acc: 100%] [G loss: 3.655563, adv: 1.013671, recon: 0.063645, id: 0.285797] time: 1:29:55.149507 \n",
            "[Epoch 42/50] [Batch 0] [D loss: 0.009469, acc: 100%] [G loss: 3.726596, adv: 1.028333, recon: 0.071282, id: 0.132363] time: 1:30:03.057514 \n",
            "[Epoch 42/50] [Batch 500] [D loss: 0.026873, acc: 100%] [G loss: 4.203531, adv: 1.065001, recon: 0.088697, id: 0.108350] time: 1:31:02.671141 \n",
            "[Epoch 42/50] [Batch 1000] [D loss: 0.024505, acc: 100%] [G loss: 4.448967, adv: 1.175389, recon: 0.093390, id: 0.079259] time: 1:32:02.346570 \n",
            "[Epoch 43/50] [Batch 0] [D loss: 0.041127, acc:  96%] [G loss: 3.341343, adv: 0.918015, recon: 0.064756, id: 0.122122] time: 1:32:10.343466 \n",
            "[Epoch 43/50] [Batch 500] [D loss: 0.029879, acc: 100%] [G loss: 3.569274, adv: 0.953543, recon: 0.075352, id: 0.067243] time: 1:33:10.176197 \n",
            "[Epoch 43/50] [Batch 1000] [D loss: 0.004289, acc: 100%] [G loss: 3.906437, adv: 1.011025, recon: 0.081583, id: 0.111687] time: 1:34:09.587422 \n",
            "[Epoch 44/50] [Batch 0] [D loss: 0.004637, acc: 100%] [G loss: 3.463074, adv: 1.016715, recon: 0.062593, id: 0.091986] time: 1:34:17.553448 \n",
            "[Epoch 44/50] [Batch 500] [D loss: 0.004612, acc: 100%] [G loss: 3.557302, adv: 0.987704, recon: 0.066141, id: 0.105101] time: 1:35:17.199080 \n",
            "[Epoch 44/50] [Batch 1000] [D loss: 0.010032, acc: 100%] [G loss: 3.919870, adv: 0.941083, recon: 0.089625, id: 0.111024] time: 1:36:16.731677 \n",
            "[Epoch 45/50] [Batch 0] [D loss: 0.010618, acc: 100%] [G loss: 3.901762, adv: 1.010226, recon: 0.078628, id: 0.185983] time: 1:36:24.703822 \n",
            "[Epoch 45/50] [Batch 500] [D loss: 0.001641, acc: 100%] [G loss: 3.604039, adv: 1.008486, recon: 0.069331, id: 0.072473] time: 1:37:24.022853 \n",
            "[Epoch 45/50] [Batch 1000] [D loss: 0.002454, acc: 100%] [G loss: 3.685549, adv: 1.053310, recon: 0.066366, id: 0.077198] time: 1:38:23.376300 \n",
            "[Epoch 46/50] [Batch 0] [D loss: 0.001659, acc: 100%] [G loss: 3.997437, adv: 0.976666, recon: 0.089299, id: 0.121316] time: 1:38:31.313368 \n",
            "[Epoch 46/50] [Batch 500] [D loss: 0.005690, acc: 100%] [G loss: 3.595343, adv: 0.998463, recon: 0.070563, id: 0.077411] time: 1:39:31.453746 \n",
            "[Epoch 46/50] [Batch 1000] [D loss: 0.002704, acc: 100%] [G loss: 3.638659, adv: 1.009849, recon: 0.066073, id: 0.076879] time: 1:40:30.887861 \n",
            "[Epoch 47/50] [Batch 0] [D loss: 0.002247, acc: 100%] [G loss: 3.546632, adv: 1.030495, recon: 0.063957, id: 0.052354] time: 1:40:38.834231 \n",
            "[Epoch 47/50] [Batch 500] [D loss: 0.004733, acc: 100%] [G loss: 3.897934, adv: 1.047272, recon: 0.077896, id: 0.162981] time: 1:41:38.222472 \n",
            "[Epoch 47/50] [Batch 1000] [D loss: 0.006690, acc: 100%] [G loss: 3.598791, adv: 0.998275, recon: 0.070249, id: 0.072661] time: 1:42:37.357522 \n",
            "[Epoch 48/50] [Batch 0] [D loss: 0.057280, acc: 100%] [G loss: 3.830390, adv: 0.942154, recon: 0.087462, id: 0.093902] time: 1:42:45.344699 \n",
            "[Epoch 48/50] [Batch 500] [D loss: 0.008454, acc: 100%] [G loss: 3.773178, adv: 0.988244, recon: 0.078957, id: 0.095202] time: 1:43:44.653219 \n",
            "[Epoch 48/50] [Batch 1000] [D loss: 0.002203, acc: 100%] [G loss: 3.714976, adv: 1.013574, recon: 0.073610, id: 0.083516] time: 1:44:43.989983 \n",
            "[Epoch 49/50] [Batch 0] [D loss: 0.002833, acc: 100%] [G loss: 3.575751, adv: 0.986664, recon: 0.067818, id: 0.128379] time: 1:44:51.926664 \n",
            "[Epoch 49/50] [Batch 500] [D loss: 0.006619, acc: 100%] [G loss: 3.640886, adv: 1.077797, recon: 0.064598, id: 0.077694] time: 1:45:51.267632 \n",
            "[Epoch 49/50] [Batch 1000] [D loss: 0.029800, acc: 100%] [G loss: 4.131849, adv: 0.971968, recon: 0.094637, id: 0.177653] time: 1:46:50.878016 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38uxiPkppk5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/gen_images/47_500.png '/content/drive/My Drive/App/CycleGAN/gen_images' \n",
        "!cp /content/gen_images/48_500.png '/content/drive/My Drive/App/CycleGAN/gen_images' \n",
        "!cp /content/gen_images/49_500.png '/content/drive/My Drive/App/CycleGAN/gen_images' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAv_YDpPqEG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}