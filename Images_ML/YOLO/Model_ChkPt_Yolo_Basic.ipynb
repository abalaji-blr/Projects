{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3C_Model_ChkPt_Yolo_Basic.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "8edSiHPi2N5l",
        "colab_type": "code",
        "outputId": "287a3354-705f-4a8b-c324-a3c7e8464aaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "#epochs = 300\n",
        "epochs = 50\n",
        "l = 10\n",
        "num_filter = 20"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-hkvQwOH2cCO",
        "colab_type": "code",
        "outputId": "24908cd0-5d3f-42da-ddf5-8123cb82e124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 178s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ztoDypc63gEs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def space_to_depth_x2(x):\n",
        "    return tf.space_to_depth(x, block_size=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qbfTasESTJQp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def space_to_depth_x8(x):\n",
        "  return tf.space_to_depth(x, block_size=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-MXS00eYpJXr",
        "colab_type": "code",
        "outputId": "dccd0ab9-c579-46de-8365-c78b5c349443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_train[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "SNnGm8Tv2fR1",
        "colab_type": "code",
        "outputId": "dbefc0c1-fe74-4bd8-e7e4-ba59d41714df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "''' \n",
        "\n",
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "\n",
        "# Layer 1\n",
        "#layer1 = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input)\n",
        "#layer1 = BatchNormalization(name='norm_1')(layer1)\n",
        "#layer1 = LeakyReLU(alpha=0.1)(layer1)\n",
        "\n",
        "layer1 = LayerBlock(input)\n",
        "layer1 = MaxPooling2D(pool_size=(2, 2))(layer1)\n",
        "\n",
        "# Layer 2\n",
        "layer2 = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(layer1)\n",
        "layer2 = BatchNormalization(name='norm_2')(layer2)\n",
        "layer2 = LeakyReLU(alpha=0.1)(layer2)\n",
        "layer2 = MaxPooling2D(pool_size=(2, 2))(layer2)\n",
        "\n",
        "# Layer 3\n",
        "layer3 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False)(layer2)\n",
        "layer3 = BatchNormalization(name='norm_3')(layer3)\n",
        "layer3 = LeakyReLU(alpha=0.1)(layer3)\n",
        "\n",
        "# Layer 4\n",
        "layer4 = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4', use_bias=False)(layer3)\n",
        "layer4 = BatchNormalization(name='norm_4')(layer4)\n",
        "layer4 = LeakyReLU(alpha=0.1)(layer4)\n",
        "\n",
        "# Layer 5\n",
        "layer5 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False)(layer4)\n",
        "layer5 = BatchNormalization(name='norm_5')(layer5)\n",
        "layer5 = LeakyReLU(alpha=0.1)(layer5)\n",
        "layer5 = MaxPooling2D(pool_size=(2, 2))(layer5)\n",
        "\n",
        "# Layer 6\n",
        "layer6 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(layer5)\n",
        "layer6 = BatchNormalization(name='norm_6')(layer6)\n",
        "layer6 = LeakyReLU(alpha=0.1)(layer6)\n",
        "\n",
        "# Layer 7\n",
        "layer7 = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(layer6)\n",
        "layer7 = BatchNormalization(name='norm_7')(layer7)\n",
        "layer7 = LeakyReLU(alpha=0.1)(layer7)\n",
        "\n",
        "# Layer 8\n",
        "layer8 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False)(layer7)\n",
        "layer8 = BatchNormalization(name='norm_8')(layer8)\n",
        "layer8 = LeakyReLU(alpha=0.1)(layer8)\n",
        "layer8 = MaxPooling2D(pool_size=(2, 2))(layer8)\n",
        "\n",
        "# Layer 9\n",
        "layer9 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(layer8)\n",
        "layer9 = BatchNormalization(name='norm_9')(layer9)\n",
        "layer9 = LeakyReLU(alpha=0.1)(layer9)\n",
        "\n",
        "# Layer 10\n",
        "layer10 = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(layer9)\n",
        "layer10 = BatchNormalization(name='norm_10')(layer10)\n",
        "layer10 = LeakyReLU(alpha=0.1)(layer10)\n",
        "\n",
        "# Layer 11\n",
        "layer11 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(layer10)\n",
        "layer11 = BatchNormalization(name='norm_11')(layer11)\n",
        "layer11 = LeakyReLU(alpha=0.1)(layer11)\n",
        "\n",
        "# Layer 12\n",
        "layer12 = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(layer11)\n",
        "layer12 = BatchNormalization(name='norm_12')(layer12)\n",
        "layer12 = LeakyReLU(alpha=0.1)(layer12)\n",
        "\n",
        "# Layer 13\n",
        "layer13 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(layer12)\n",
        "layer13 = BatchNormalization(name='norm_13')(layer13)\n",
        "layer13_act = LeakyReLU(alpha=0.1)(layer13)\n",
        "\n",
        "#skip_connection = layer13_act\n",
        "skip_connection  = layer12\n",
        "\n",
        "layer13 = MaxPooling2D(pool_size=(2, 2))(layer13_act)\n",
        "\n",
        "# Layer 14\n",
        "layer14 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(layer13)\n",
        "layer14 = BatchNormalization(name='norm_14')(layer14)\n",
        "layer14 = LeakyReLU(alpha=0.1)(layer14)\n",
        "\n",
        "# Layer 15\n",
        "layer15 = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(layer14)\n",
        "layer15 = BatchNormalization(name='norm_15')(layer15)\n",
        "layer15 = LeakyReLU(alpha=0.1)(layer15)\n",
        "\n",
        "# Layer 16\n",
        "layer16 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(layer15)\n",
        "layer16 = BatchNormalization(name='norm_16')(layer16)\n",
        "layer16 = LeakyReLU(alpha=0.1)(layer16)\n",
        "\n",
        "# Layer 17\n",
        "layer17 = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(layer16)\n",
        "layer17 = BatchNormalization(name='norm_17')(layer17)\n",
        "layer17 = LeakyReLU(alpha=0.1)(layer17)\n",
        "\n",
        "# Layer 18\n",
        "layer18 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(layer17)\n",
        "layer18 = BatchNormalization(name='norm_18')(layer18)\n",
        "layer18 = LeakyReLU(alpha=0.1)(layer18)\n",
        "\n",
        "# Layer 19\n",
        "layer19 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(layer18)\n",
        "layer19 = BatchNormalization(name='norm_19')(layer19)\n",
        "layer19 = LeakyReLU(alpha=0.1)(layer19)\n",
        "\n",
        "# Layer 20\n",
        "layer20 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(layer19)\n",
        "layer20 = BatchNormalization(name='norm_20')(layer20)\n",
        "layer20 = LeakyReLU(alpha=0.1)(layer20)\n",
        "\n",
        "# Layer 21\n",
        "skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
        "skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
        "skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
        "skip_connection = Lambda(space_to_depth_x2)(skip_connection)\n",
        "\n",
        "layer21 = concatenate([skip_connection, layer20])\n",
        "\n",
        "# Layer 22\n",
        "layer22 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(layer21)\n",
        "layer22 = BatchNormalization(name='norm_22')(layer22)\n",
        "layer22 = LeakyReLU(alpha=0.1)(layer22)\n",
        "\n",
        "# Layer 23\n",
        "layer23 = Flatten()(layer22)\n",
        "\n",
        "output = Dense(num_classes, activation='softmax')(layer23)\n",
        "\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" \\n\\ninput = Input(shape=(img_height, img_width, channel,))\\n\\n# Layer 1\\n#layer1 = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input)\\n#layer1 = BatchNormalization(name='norm_1')(layer1)\\n#layer1 = LeakyReLU(alpha=0.1)(layer1)\\n\\nlayer1 = LayerBlock(input)\\nlayer1 = MaxPooling2D(pool_size=(2, 2))(layer1)\\n\\n# Layer 2\\nlayer2 = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(layer1)\\nlayer2 = BatchNormalization(name='norm_2')(layer2)\\nlayer2 = LeakyReLU(alpha=0.1)(layer2)\\nlayer2 = MaxPooling2D(pool_size=(2, 2))(layer2)\\n\\n# Layer 3\\nlayer3 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False)(layer2)\\nlayer3 = BatchNormalization(name='norm_3')(layer3)\\nlayer3 = LeakyReLU(alpha=0.1)(layer3)\\n\\n# Layer 4\\nlayer4 = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4', use_bias=False)(layer3)\\nlayer4 = BatchNormalization(name='norm_4')(layer4)\\nlayer4 = LeakyReLU(alpha=0.1)(layer4)\\n\\n# Layer 5\\nlayer5 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False)(layer4)\\nlayer5 = BatchNormalization(name='norm_5')(layer5)\\nlayer5 = LeakyReLU(alpha=0.1)(layer5)\\nlayer5 = MaxPooling2D(pool_size=(2, 2))(layer5)\\n\\n# Layer 6\\nlayer6 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(layer5)\\nlayer6 = BatchNormalization(name='norm_6')(layer6)\\nlayer6 = LeakyReLU(alpha=0.1)(layer6)\\n\\n# Layer 7\\nlayer7 = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(layer6)\\nlayer7 = BatchNormalization(name='norm_7')(layer7)\\nlayer7 = LeakyReLU(alpha=0.1)(layer7)\\n\\n# Layer 8\\nlayer8 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False)(layer7)\\nlayer8 = BatchNormalization(name='norm_8')(layer8)\\nlayer8 = LeakyReLU(alpha=0.1)(layer8)\\nlayer8 = MaxPooling2D(pool_size=(2, 2))(layer8)\\n\\n# Layer 9\\nlayer9 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(layer8)\\nlayer9 = BatchNormalization(name='norm_9')(layer9)\\nlayer9 = LeakyReLU(alpha=0.1)(layer9)\\n\\n# Layer 10\\nlayer10 = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(layer9)\\nlayer10 = BatchNormalization(name='norm_10')(layer10)\\nlayer10 = LeakyReLU(alpha=0.1)(layer10)\\n\\n# Layer 11\\nlayer11 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(layer10)\\nlayer11 = BatchNormalization(name='norm_11')(layer11)\\nlayer11 = LeakyReLU(alpha=0.1)(layer11)\\n\\n# Layer 12\\nlayer12 = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(layer11)\\nlayer12 = BatchNormalization(name='norm_12')(layer12)\\nlayer12 = LeakyReLU(alpha=0.1)(layer12)\\n\\n# Layer 13\\nlayer13 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(layer12)\\nlayer13 = BatchNormalization(name='norm_13')(layer13)\\nlayer13_act = LeakyReLU(alpha=0.1)(layer13)\\n\\n#skip_connection = layer13_act\\nskip_connection  = layer12\\n\\nlayer13 = MaxPooling2D(pool_size=(2, 2))(layer13_act)\\n\\n# Layer 14\\nlayer14 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(layer13)\\nlayer14 = BatchNormalization(name='norm_14')(layer14)\\nlayer14 = LeakyReLU(alpha=0.1)(layer14)\\n\\n# Layer 15\\nlayer15 = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(layer14)\\nlayer15 = BatchNormalization(name='norm_15')(layer15)\\nlayer15 = LeakyReLU(alpha=0.1)(layer15)\\n\\n# Layer 16\\nlayer16 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(layer15)\\nlayer16 = BatchNormalization(name='norm_16')(layer16)\\nlayer16 = LeakyReLU(alpha=0.1)(layer16)\\n\\n# Layer 17\\nlayer17 = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(layer16)\\nlayer17 = BatchNormalization(name='norm_17')(layer17)\\nlayer17 = LeakyReLU(alpha=0.1)(layer17)\\n\\n# Layer 18\\nlayer18 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(layer17)\\nlayer18 = BatchNormalization(name='norm_18')(layer18)\\nlayer18 = LeakyReLU(alpha=0.1)(layer18)\\n\\n# Layer 19\\nlayer19 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(layer18)\\nlayer19 = BatchNormalization(name='norm_19')(layer19)\\nlayer19 = LeakyReLU(alpha=0.1)(layer19)\\n\\n# Layer 20\\nlayer20 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(layer19)\\nlayer20 = BatchNormalization(name='norm_20')(layer20)\\nlayer20 = LeakyReLU(alpha=0.1)(layer20)\\n\\n# Layer 21\\nskip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\\nskip_connection = BatchNormalization(name='norm_21')(skip_connection)\\nskip_connection = LeakyReLU(alpha=0.1)(skip_connection)\\nskip_connection = Lambda(space_to_depth_x2)(skip_connection)\\n\\nlayer21 = concatenate([skip_connection, layer20])\\n\\n# Layer 22\\nlayer22 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(layer21)\\nlayer22 = BatchNormalization(name='norm_22')(layer22)\\nlayer22 = LeakyReLU(alpha=0.1)(layer22)\\n\\n# Layer 23\\nlayer23 = Flatten()(layer22)\\n\\noutput = Dense(num_classes, activation='softmax')(layer23)\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "NAZBcIo0HPZy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def LayerBlock(layer_in, block_name):\n",
        "  \n",
        "  conv_prefix = block_name + '_conv_'\n",
        "  norm_prefix = block_name + '_norm_'\n",
        "  \n",
        "  # layer1\n",
        "  layer1 = Conv2D(32, (3,3), strides=(1,1), padding='same', name=conv_prefix+'1', use_bias=False)(layer_in)\n",
        "  layer1 = BatchNormalization(name=norm_prefix+'1')(layer1)\n",
        "  layer1 = LeakyReLU(alpha=0.1)(layer1)\n",
        "\n",
        "  # Layer 2\n",
        "  layer2 = Conv2D(64, (3,3), strides=(1,1), padding='same', name=conv_prefix+'2', use_bias=False)(layer1)\n",
        "  layer2 = BatchNormalization(name=norm_prefix+'2')(layer2)\n",
        "  layer2 = LeakyReLU(alpha=0.1)(layer2)\n",
        "  \n",
        "  # Layer 3\n",
        "  layer3 = Conv2D(128, (3,3), strides=(1,1), padding='same', name=conv_prefix+'3', use_bias=False)(layer2)\n",
        "  layer3 = BatchNormalization(name=norm_prefix+'3')(layer3)\n",
        "  layer3 = LeakyReLU(alpha=0.1)(layer3)\n",
        "  \n",
        "  # Layer 4\n",
        "  layer4 = Conv2D(256, (3,3), strides=(1,1), padding='same', name=conv_prefix+'4', use_bias=False)(layer3)\n",
        "  layer4 = BatchNormalization(name=norm_prefix+'4')(layer4)\n",
        "  layer4 = LeakyReLU(alpha=0.1)(layer4)\n",
        "\n",
        "  # Layer 5\n",
        "  layer5 = Conv2D(512, (3,3), strides=(1,1), padding='same', name=conv_prefix+'5', use_bias=False)(layer4)\n",
        "  layer5 = BatchNormalization(name=norm_prefix+'5')(layer5)\n",
        "  layer5 = LeakyReLU(alpha=0.1)(layer5)\n",
        "  \n",
        "  # MaxPooling\n",
        "  out_layer = MaxPooling2D(pool_size=(2, 2))(layer5)\n",
        "  \n",
        "  return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Z6J_-RPKz3l",
        "colab_type": "code",
        "outputId": "b0a12506-e8f8-462b-d2e9-2537621360a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "\n",
        "block1 = LayerBlock(input, 'A')\n",
        "\n",
        "print('block1:')\n",
        "print(block1.shape)\n",
        "\n",
        "block2 = LayerBlock(block1, 'B')\n",
        "\n",
        "block3 = LayerBlock(block2, 'C')\n",
        "\n",
        "block4 = LayerBlock(block3, 'D')\n",
        "\n",
        "print('block4')\n",
        "print(block4.shape)\n",
        "\n",
        "####### concat block1 & block4\n",
        "## reshape block1\n",
        "block1_reshape = Lambda(space_to_depth_x8)(block1)\n",
        "\n",
        "concat_layer = concatenate([block1_reshape, block4])\n",
        "\n",
        "print('concat_layer')\n",
        "print(concat_layer.shape)\n",
        "\n",
        "############\n",
        "## connect the concat layer to softmax thru conv layer\n",
        "#########\n",
        "layer_final = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_final', use_bias=False)(concat_layer)\n",
        "layer_final = BatchNormalization(name='norm_final')(layer_final)\n",
        "layer_final = LeakyReLU(alpha=0.1)(layer_final)\n",
        "print('layer_final')\n",
        "print(layer_final.shape)\n",
        "\n",
        "##layer_final = Conv2D(10, (1,1), name='conv_final', use_bias=False)(concat_layer)\n",
        "\n",
        "layer_end   = Conv2D(10, (2,2))(layer_final)\n",
        "\n",
        "flatten_layer = Flatten()(layer_end)\n",
        "output = Activation('softmax')(flatten_layer)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "block1:\n",
            "(?, 16, 16, 512)\n",
            "block4\n",
            "(?, 2, 2, 512)\n",
            "concat_layer\n",
            "(?, 2, 2, 33280)\n",
            "layer_final\n",
            "(?, 2, 2, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jeh0VAxy26NV",
        "colab_type": "code",
        "outputId": "2a2db455-441d-4baf-97eb-c7d076eb9f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2635
        }
      },
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "A_conv_1 (Conv2D)               (None, 32, 32, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "A_norm_1 (BatchNormalization)   (None, 32, 32, 32)   128         A_conv_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           A_norm_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "A_conv_2 (Conv2D)               (None, 32, 32, 64)   18432       leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "A_norm_2 (BatchNormalization)   (None, 32, 32, 64)   256         A_conv_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 32, 32, 64)   0           A_norm_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "A_conv_3 (Conv2D)               (None, 32, 32, 128)  73728       leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "A_norm_3 (BatchNormalization)   (None, 32, 32, 128)  512         A_conv_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 32, 32, 128)  0           A_norm_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "A_conv_4 (Conv2D)               (None, 32, 32, 256)  294912      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "A_norm_4 (BatchNormalization)   (None, 32, 32, 256)  1024        A_conv_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 32, 32, 256)  0           A_norm_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "A_conv_5 (Conv2D)               (None, 32, 32, 512)  1179648     leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "A_norm_5 (BatchNormalization)   (None, 32, 32, 512)  2048        A_conv_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 32, 32, 512)  0           A_norm_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 512)  0           leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "B_conv_1 (Conv2D)               (None, 16, 16, 32)   147456      max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "B_norm_1 (BatchNormalization)   (None, 16, 16, 32)   128         B_conv_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 16, 16, 32)   0           B_norm_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "B_conv_2 (Conv2D)               (None, 16, 16, 64)   18432       leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "B_norm_2 (BatchNormalization)   (None, 16, 16, 64)   256         B_conv_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 16, 16, 64)   0           B_norm_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "B_conv_3 (Conv2D)               (None, 16, 16, 128)  73728       leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "B_norm_3 (BatchNormalization)   (None, 16, 16, 128)  512         B_conv_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 128)  0           B_norm_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "B_conv_4 (Conv2D)               (None, 16, 16, 256)  294912      leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "B_norm_4 (BatchNormalization)   (None, 16, 16, 256)  1024        B_conv_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 16, 16, 256)  0           B_norm_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "B_conv_5 (Conv2D)               (None, 16, 16, 512)  1179648     leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "B_norm_5 (BatchNormalization)   (None, 16, 16, 512)  2048        B_conv_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 16, 16, 512)  0           B_norm_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 512)    0           leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "C_conv_1 (Conv2D)               (None, 8, 8, 32)     147456      max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "C_norm_1 (BatchNormalization)   (None, 8, 8, 32)     128         C_conv_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 8, 8, 32)     0           C_norm_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "C_conv_2 (Conv2D)               (None, 8, 8, 64)     18432       leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "C_norm_2 (BatchNormalization)   (None, 8, 8, 64)     256         C_conv_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 8, 8, 64)     0           C_norm_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "C_conv_3 (Conv2D)               (None, 8, 8, 128)    73728       leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "C_norm_3 (BatchNormalization)   (None, 8, 8, 128)    512         C_conv_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)      (None, 8, 8, 128)    0           C_norm_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "C_conv_4 (Conv2D)               (None, 8, 8, 256)    294912      leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "C_norm_4 (BatchNormalization)   (None, 8, 8, 256)    1024        C_conv_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)      (None, 8, 8, 256)    0           C_norm_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "C_conv_5 (Conv2D)               (None, 8, 8, 512)    1179648     leaky_re_lu_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "C_norm_5 (BatchNormalization)   (None, 8, 8, 512)    2048        C_conv_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)      (None, 8, 8, 512)    0           C_norm_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 512)    0           leaky_re_lu_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "D_conv_1 (Conv2D)               (None, 4, 4, 32)     147456      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "D_norm_1 (BatchNormalization)   (None, 4, 4, 32)     128         D_conv_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)      (None, 4, 4, 32)     0           D_norm_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "D_conv_2 (Conv2D)               (None, 4, 4, 64)     18432       leaky_re_lu_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "D_norm_2 (BatchNormalization)   (None, 4, 4, 64)     256         D_conv_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)      (None, 4, 4, 64)     0           D_norm_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "D_conv_3 (Conv2D)               (None, 4, 4, 128)    73728       leaky_re_lu_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "D_norm_3 (BatchNormalization)   (None, 4, 4, 128)    512         D_conv_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)      (None, 4, 4, 128)    0           D_norm_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "D_conv_4 (Conv2D)               (None, 4, 4, 256)    294912      leaky_re_lu_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "D_norm_4 (BatchNormalization)   (None, 4, 4, 256)    1024        D_conv_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, 4, 4, 256)    0           D_norm_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "D_conv_5 (Conv2D)               (None, 4, 4, 512)    1179648     leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "D_norm_5 (BatchNormalization)   (None, 4, 4, 512)    2048        D_conv_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)      (None, 4, 4, 512)    0           D_norm_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 2, 2, 32768)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 2, 2, 512)    0           leaky_re_lu_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 2, 2, 33280)  0           lambda_1[0][0]                   \n",
            "                                                                 max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv_final (Conv2D)             (None, 2, 2, 32)     9584640     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_final (BatchNormalization) (None, 2, 2, 32)     128         conv_final[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, 2, 2, 32)     0           norm_final[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 1, 1, 10)     1290        leaky_re_lu_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 10)           0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 10)           0           flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 16,312,042\n",
            "Trainable params: 16,304,042\n",
            "Non-trainable params: 8,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BfZKE7THGpH0",
        "colab_type": "code",
        "outputId": "62fe35b2-3a6a-46fd-9bb5-da15078040f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install graphviz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (0.10.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5V9Z5rLqGM2v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KXCNoCvDHsEX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFHaCnDIGP-j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_model(model, show_shapes=True, to_file='model2.png')\n",
        "#plot_model(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zXKA4hMULToS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('model2.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "apCwOjvZ4Kts",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yFn-aZ4CbcIn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## to save the best model \n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e1az1ZJQbx_W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "?ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ewTg4766bmie",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint('3C_best_model.hdf5', monitor='val_acc', verbose=1, save_best_only=True)\n",
        "\n",
        "callbacks = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tLaFy2AO4TLl",
        "colab_type": "code",
        "outputId": "28b7f405-34dd-4aff-cbf2-24c94d4e4375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3522
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 288s 6ms/step - loss: 1.2898 - acc: 0.5425 - val_loss: 2.0136 - val_acc: 0.4564\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.45640, saving model to 3C_best_model.hdf5\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 272s 5ms/step - loss: 0.8088 - acc: 0.7166 - val_loss: 1.0581 - val_acc: 0.6498\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.45640 to 0.64980, saving model to 3C_best_model.hdf5\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.6366 - acc: 0.7790 - val_loss: 1.0978 - val_acc: 0.6473\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.64980\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.5229 - acc: 0.8168 - val_loss: 0.7821 - val_acc: 0.7382\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.64980 to 0.73820, saving model to 3C_best_model.hdf5\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.4105 - acc: 0.8580 - val_loss: 0.8834 - val_acc: 0.7144\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.73820\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.3121 - acc: 0.8944 - val_loss: 0.9492 - val_acc: 0.7212\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.73820\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.2192 - acc: 0.9275 - val_loss: 0.7170 - val_acc: 0.7759\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.73820 to 0.77590, saving model to 3C_best_model.hdf5\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.1486 - acc: 0.9514 - val_loss: 0.9319 - val_acc: 0.7513\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.77590\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0952 - acc: 0.9719 - val_loss: 0.7966 - val_acc: 0.7775\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.77590 to 0.77750, saving model to 3C_best_model.hdf5\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0588 - acc: 0.9845 - val_loss: 1.0427 - val_acc: 0.7557\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.77750\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0487 - acc: 0.9861 - val_loss: 1.5068 - val_acc: 0.7114\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.77750\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0580 - acc: 0.9824 - val_loss: 0.9867 - val_acc: 0.7781\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.77750 to 0.77810, saving model to 3C_best_model.hdf5\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0450 - acc: 0.9860 - val_loss: 1.0070 - val_acc: 0.7733\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.77810\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0501 - acc: 0.9839 - val_loss: 1.1952 - val_acc: 0.7538\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.77810\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0376 - acc: 0.9887 - val_loss: 0.9702 - val_acc: 0.7830\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.77810 to 0.78300, saving model to 3C_best_model.hdf5\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0240 - acc: 0.9927 - val_loss: 1.0098 - val_acc: 0.7885\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.78300 to 0.78850, saving model to 3C_best_model.hdf5\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0262 - acc: 0.9922 - val_loss: 1.2795 - val_acc: 0.7554\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.78850\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0240 - acc: 0.9929 - val_loss: 1.3978 - val_acc: 0.7297\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.78850\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0427 - acc: 0.9854 - val_loss: 1.1565 - val_acc: 0.7701\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.78850\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 272s 5ms/step - loss: 0.0314 - acc: 0.9899 - val_loss: 1.1397 - val_acc: 0.7719\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.78850\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0103 - acc: 0.9973 - val_loss: 0.9841 - val_acc: 0.8053\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.78850 to 0.80530, saving model to 3C_best_model.hdf5\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0089 - acc: 0.9979 - val_loss: 1.0493 - val_acc: 0.7790\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.80530\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0212 - acc: 0.9934 - val_loss: 1.2951 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.80530\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0480 - acc: 0.9837 - val_loss: 1.0166 - val_acc: 0.7956\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.80530\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0139 - acc: 0.9962 - val_loss: 1.0329 - val_acc: 0.7956\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.80530\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0036 - acc: 0.9995 - val_loss: 0.9929 - val_acc: 0.8067\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.80530 to 0.80670, saving model to 3C_best_model.hdf5\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.9592 - val_acc: 0.8155\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.80670 to 0.81550, saving model to 3C_best_model.hdf5\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 3.0733e-04 - acc: 1.0000 - val_loss: 0.9577 - val_acc: 0.8199\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.81550 to 0.81990, saving model to 3C_best_model.hdf5\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 2.4410e-04 - acc: 1.0000 - val_loss: 0.9617 - val_acc: 0.8214\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.81990 to 0.82140, saving model to 3C_best_model.hdf5\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 1.5826e-04 - acc: 1.0000 - val_loss: 0.9687 - val_acc: 0.8208\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.82140\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 272s 5ms/step - loss: 1.2323e-04 - acc: 1.0000 - val_loss: 0.9712 - val_acc: 0.8215\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.82140 to 0.82150, saving model to 3C_best_model.hdf5\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 1.0588e-04 - acc: 1.0000 - val_loss: 0.9849 - val_acc: 0.8199\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.82150\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 8.3916e-05 - acc: 1.0000 - val_loss: 0.9819 - val_acc: 0.8214\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.82150\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 7.0003e-05 - acc: 1.0000 - val_loss: 0.9861 - val_acc: 0.8216\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.82150 to 0.82160, saving model to 3C_best_model.hdf5\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 5.8299e-05 - acc: 1.0000 - val_loss: 0.9944 - val_acc: 0.8227\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.82160 to 0.82270, saving model to 3C_best_model.hdf5\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 6.8108e-05 - acc: 1.0000 - val_loss: 1.0001 - val_acc: 0.8216\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.82270\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 4.4105e-05 - acc: 1.0000 - val_loss: 1.0154 - val_acc: 0.8216\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.82270\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 3.6127e-05 - acc: 1.0000 - val_loss: 1.0144 - val_acc: 0.8224\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.82270\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 272s 5ms/step - loss: 2.7778e-05 - acc: 1.0000 - val_loss: 1.0229 - val_acc: 0.8232\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.82270 to 0.82320, saving model to 3C_best_model.hdf5\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 2.3189e-05 - acc: 1.0000 - val_loss: 1.0264 - val_acc: 0.8241\n",
            "\n",
            "Epoch 00040: val_acc improved from 0.82320 to 0.82410, saving model to 3C_best_model.hdf5\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 1.9442e-05 - acc: 1.0000 - val_loss: 1.0332 - val_acc: 0.8236\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.82410\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 272s 5ms/step - loss: 1.6440e-05 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 0.8231\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.82410\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 1.5601e-05 - acc: 1.0000 - val_loss: 1.0544 - val_acc: 0.8234\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.82410\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 272s 5ms/step - loss: 1.1163e-05 - acc: 1.0000 - val_loss: 1.0567 - val_acc: 0.8233\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.82410\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 9.2545e-06 - acc: 1.0000 - val_loss: 1.0676 - val_acc: 0.8247\n",
            "\n",
            "Epoch 00045: val_acc improved from 0.82410 to 0.82470, saving model to 3C_best_model.hdf5\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 272s 5ms/step - loss: 7.6253e-06 - acc: 1.0000 - val_loss: 1.0721 - val_acc: 0.8246\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.82470\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 272s 5ms/step - loss: 6.4793e-06 - acc: 1.0000 - val_loss: 1.0816 - val_acc: 0.8253\n",
            "\n",
            "Epoch 00047: val_acc improved from 0.82470 to 0.82530, saving model to 3C_best_model.hdf5\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 272s 5ms/step - loss: 5.3511e-06 - acc: 1.0000 - val_loss: 1.0866 - val_acc: 0.8266\n",
            "\n",
            "Epoch 00048: val_acc improved from 0.82530 to 0.82660, saving model to 3C_best_model.hdf5\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 4.3354e-06 - acc: 1.0000 - val_loss: 1.0926 - val_acc: 0.8267\n",
            "\n",
            "Epoch 00049: val_acc improved from 0.82660 to 0.82670, saving model to 3C_best_model.hdf5\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 3.5711e-06 - acc: 1.0000 - val_loss: 1.1043 - val_acc: 0.8264\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.82670\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd8ee957c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "pTfZIGrf4Uyd",
        "colab_type": "code",
        "outputId": "c70fe62d-4859-4744-9c53-5cc4b7cffd88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "# Save the trained weights in to .h5 format\n",
        "model.save_weights(\"Yolo_Basic_model2.h5\")\n",
        "print(\"Saved the model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 23s 2ms/step\n",
            "Test loss: 1.1042507146820426\n",
            "Test accuracy: 0.8264\n",
            "Saved the model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qCf_tiWZ2iBI",
        "colab_type": "code",
        "outputId": "884c6d65-3d83-4c79-d616-1fbc1b858531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jiyb9TlVGsZs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save_weights(\"Yolo_Basic_model2.h5\")\n",
        "print(\"Saved the model to disk\")\n",
        "from google.colab import files\n",
        "\n",
        "#files.download('/content/Yolo_Basic_model2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g9A3pesKbUJ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}