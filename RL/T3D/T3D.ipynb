{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T3D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-8P4acC2Yvt",
        "colab_type": "text"
      },
      "source": [
        "# T3D Implementation\n",
        "\n",
        "---\n",
        "* Twin Delayed Deep Deterministic Policy Gradient\n",
        "  * **Policy** is the probability distribution of **actions** for a given state.\n",
        "  * The *policy* is what agent controls. \n",
        "    * When the agent follows a policy, it generates a sequence of states, actions and rewards. \n",
        "    * It called as *trajectory*.\n",
        "\n",
        "  * **Policy Gradient**\n",
        "    * The objective of the **reinforcment learning agent** is to maximize the *(discounted) reward (from the start state)* when following the *policy*.\n",
        "    * In ML setup, we define a set of parameters ($\\theta$) to parmeterize the *policy*.\n",
        "    * The objective is to *maximize* the \"expected\" reward following a parameterize policy.\n",
        "    * Atleast there will be one optimal policy which can give *maximum* reward. \n",
        "    * Among all optimal policies, atleast there will be one, which is **stationary and deterministic**.\n",
        "    * In ML, to maximize, we need to do **Gradient Ascent**.\n",
        "    * \n",
        "\n",
        "  * **Deterministic Policy Gradient**\n",
        "    * Learn a *deterministic action* for a given state.\n",
        "    * Use **Actor-Critic Model**\n",
        "\n",
        "  * **Deep Deterministic Policy Gradient (DDPG)**\n",
        "    * The Actor and Critic are DNNs.\n",
        "\n",
        "  * Architecutre:\n",
        "    * For stability, we have **Dual** (two) Network\n",
        "      * Model- Model\n",
        "      * Model- Target\n",
        "    * Each Model is a **Duel** network.\n",
        "      * Model- Actor\n",
        "      * Uses Two Critics (thus, TWIN in the algo. name).\n",
        "      * Model- Critic1 & Critic2 (Yes, two Critics/ Twin).\n",
        "\n",
        "  * **Delayed**\n",
        "    * The model - *Model* is updated at every step. But, the model - *target* is updated once every two steps.\n",
        "\n",
        "  * **Twin**\n",
        "    * Two critics instead of single critic in the Actor-Critic model.\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inB6B0wIanVk",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "1. [Deterministic Policy Gradient Algorithms - David Silver](http://proceedings.mlr.press/v32/silver14.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCUGjlMAytcH",
        "colab_type": "text"
      },
      "source": [
        "## Generic Reinforcement Learning Algorithm\n",
        "\n",
        "---\n",
        "```\n",
        "Loop:\n",
        "    Collect trajectories (transitions - (state, action, reward, next state, terminated flag))\n",
        "    (Optionally) store trajectories in a replay buffer for sampling\n",
        "    Loop:\n",
        "        Sample a mini batch of transitions\n",
        "        Compute Policy Gradient\n",
        "        (Optionally) Compute Critic Gradient\n",
        "        Update parameters\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98C_4eNWzPP_",
        "colab_type": "text"
      },
      "source": [
        "## T3D Implementations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Qfka-qzlEJ",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zyz3eaY2UqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# general imports\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# gym\n",
        "import gym\n",
        "from gym import wrappers\n",
        "\n",
        "#\n",
        "#import pybullet_envs\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlDBey1LzoAZ",
        "colab_type": "text"
      },
      "source": [
        "### Step1: Experience Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwUE8rLrzrSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# create cyclic buffer with capacity of 1 mil.\n",
        "# to store transitions.\n",
        "#\n",
        "class ReplayBuffer(object):\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = [] \n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0 # idx to add things to buffer.\n",
        "\n",
        "  # add transition to the replay buffer\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      # buffer full, add it to the begining\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      # don't use ptr instead use append,\n",
        "      # as memory is allocated on the fly.\n",
        "      self.storage.append(transition)\n",
        "  \n",
        "\n",
        "  # sample the transitions from the buffer\n",
        "  def sample(self, batch_size):\n",
        "    # get the indices of the transitions\n",
        "    ind = np.random.randint(0, len(self.storage), batch_size)\n",
        "\n",
        "    # get the transitions for the batch \n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind:\n",
        "      #unpack\n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "\n",
        "      # why to reshape the rewards & done?\n",
        "      return np.array(batch_states), \\\n",
        "              np.array(batch_next_states), \\\n",
        "              np.array(batch_actions), \\\n",
        "              np.array(batch_rewards).reshape(-1,1), \\\n",
        "              np.array(batch_dones).reshape(-1,1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_4kUtUkK6Xn",
        "colab_type": "text"
      },
      "source": [
        "## Step2: Build DNN for Actor\n",
        "\n",
        "Note that the Actor Model and Actor Target are similar DNNs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR2-8-6MZ3i1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input: state\n",
        "# output: action\n",
        "class Actor(nn.Module):\n",
        "  def __init__(self, state_dims, action_dim, max_action):\n",
        "    # max_action is to clip in case we added too much noise\n",
        "    super(Actor, self).__init__() # init the inherited base class\n",
        "    # build the layers of NN\n",
        "    self.layer_1 = nn.Linear(state_dims, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAOSY2HoNflT",
        "colab_type": "text"
      },
      "source": [
        "## Step3: Build Critic Model\n",
        "\n",
        "Note that there are TWIN critics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3lkpw-bNn5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TwoCritics\n",
        "#\n",
        "# input: state & action\n",
        "# output: q-value\n",
        "#\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self, state_dims, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # first critic\n",
        "    self.layer_1 = nn.Linear(state_dims + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        " \n",
        "    # second critic\n",
        "    self.layer_4 = nn.Linear(state_dims + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, action_dim)  \n",
        "\n",
        "  def forward(self, x, u ): # x - state, u - action\n",
        "    xu = torch.cat([x, u], 1) # 1 for vert concatenation\n",
        "    # forward prop of first critic\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # forward prop of second critic\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "\n",
        "    return x1, x2\n",
        "\n",
        "  # this is used to update Q-values\n",
        "  def Q1(self, x, u): # x - state, u - action\n",
        "    xu = torch.cat([x,u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "\n",
        "    return(x1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWbuI8srXzyk",
        "colab_type": "text"
      },
      "source": [
        "## Step 4 thru 15: Build T3D Model and Training Procedure\n",
        "\n",
        "* Two sets of Actor-Critic Model\n",
        "  * Model- Model\n",
        "  * Model - Target\n",
        "\n",
        "---\n",
        "* Step 4: Sample batch of transitions from replay memory\n",
        "* Step 5: Find next_action (**a'**) using next_state (**s'**) . Use target_model.\n",
        "* Step 6: Add Gausian noise to next_action and clamp it to a range of values.\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmBqloqDXzhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select the device - cpu or gpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVv-l9EceUJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Build T3D model\n",
        "\n",
        "# input: state & action\n",
        "class T3D(object):\n",
        "  def __init__(self, state_dims, action_dim, max_action):\n",
        "    # build T3D class\n",
        "    self.actor = Actor(state_dims, action_dim, max_action).to(device) # Gradient Descent\n",
        "    self.actor_target = Actor(state_dims, action_dim, max_action).to(device) # polyak averaging for stability purpose\n",
        "    # initializing the model weights to keep the same.\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict)\n",
        "\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters)\n",
        "    # \n",
        "    # Critic\n",
        "    self.critic = Critic(state_dims, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dims, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict)\n",
        "\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters)\n",
        "\n",
        "    self.max_action = max_action\n",
        "\n",
        "  # input : state\n",
        "  # output : action\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    # need to return numpy\n",
        "    return self.actor(state).cpu.data.numpy().flatten()\n",
        "\n",
        "  # for each episode\n",
        "  #    sample batch of transitions (step4)\n",
        "  #\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, \\\n",
        "            tau = 0.005, policy_noise=0.2, noise_clip=0.5, policy_freq = 2):\n",
        "    for it in range(iterations):\n",
        "      # step 4: sample batch of trasitions (s, s', a, r) from replay memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones \\\n",
        "          = replay_buffer.sample(batch_size)\n",
        "      # convert to torch tensors\n",
        "      state      = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action     = torch.Tensor(batch_actions).to(device)\n",
        "      reward     = torch.Tensor(batch_rewards).to(device)\n",
        "      done       = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "      # step5: find next_action using next_state. Use actor_target model\n",
        "      # why not predict?\n",
        "      next_action = self.actor_target.forward(next_state)\n",
        "\n",
        "      # step6: Add Gausian noise to next_action and clamp it to a range of values\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "      # step7: Find Q values from the two target critics.\n",
        "      target_Q1, target_Q2 = self.critic_target.forward(next_state, next_action)\n",
        "\n",
        "      # step8: Use min of two q values. This is done to improve the stability of the model.\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      # step9: Find target_Q value: reward + gamma * min(Qt1, Qt2)\n",
        "      target_Q = reward + ( (1- done) * discount * target_Q).detach()\n",
        "\n",
        "      # step10: Find q-value from the critic models.\n",
        "      current_Q1, current_Q2 = self.critic.forward(state, action)\n",
        "\n",
        "      # step11: Compute critic loss.\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      # step12: Backpropagate the critic loss and update the params of two Critic models.\n",
        "      self.critic_optimizer.zero_grad() # init the gradients to zero\n",
        "      critic_loss.backward() # compute gradients\n",
        "      self.critic_optimizer.step() # perform weight updates\n",
        "\n",
        "      # Delayed Policy Updates\n",
        "      # every two iterations, update the models\n",
        "      if it % policy_freq == 0 :\n",
        "        # this is DPG part\n",
        "        # step13: every two iterations / episodes, update the actor model by\n",
        "        #         perfroming gradient ASCENT on the output of the first critic model\n",
        "        #\n",
        "        # negative loss is the gradient ASCENT or maximization\n",
        "        actor_loss = -(self.critic.Q1(state, self.actor(state)).mean())\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # step14: Update the frozen target_actor model\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        # step 15 : Update the frozen target_critic model\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}