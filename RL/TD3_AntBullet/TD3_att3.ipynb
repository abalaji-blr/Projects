{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3_att3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf",
        "colab_type": "text"
      },
      "source": [
        "# Twin-Delayed DDPG\n",
        "\n",
        "Complete credit goes to this [awesome Deep Reinforcement Learning 2.0 Course on Udemy](https://www.udemy.com/course/deep-reinforcement-learning/) for the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J",
        "colab_type": "text"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0",
        "colab_type": "code",
        "outputId": "6296affa-14ea-44f7-d89a-b0afb8c0ab46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/ac/a422ab8d1c57ab3f43e573b5a5f532e6afd348d81308fe66a1ecb691548e/pybullet-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (95.0MB)\n",
            "\u001b[K     |████████████████████████████████| 95.0MB 47kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-2.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av",
        "colab_type": "text"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW",
        "colab_type": "text"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex",
        "colab_type": "text"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U",
        "colab_type": "text"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP",
        "colab_type": "text"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab_type": "code",
        "outputId": "114cc73a-f044-4cd1-b66f-2eed673100ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O",
        "colab_type": "text"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z",
        "colab_type": "text"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab_type": "code",
        "outputId": "5c913180-b121-46e5-aa62-9a2ff9277f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh",
        "colab_type": "text"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem",
        "colab_type": "text"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh",
        "colab_type": "text"
      },
      "source": [
        "[link text](https://)## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s",
        "colab_type": "text"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab_type": "code",
        "outputId": "28e6b9c4-0f96-48ff-cf20-710070370291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.804960\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE",
        "colab_type": "text"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm",
        "colab_type": "text"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab_type": "code",
        "outputId": "6a381b07-ef64-4de8-fea2-bfc4eaa354c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_timesteps = 500000\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 490.78100735197626\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: 473.71296556830947\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: 512.7225612151318\n",
            "Total Timesteps: 4000 Episode Num: 4 Reward: 509.4432670210982\n",
            "Total Timesteps: 5000 Episode Num: 5 Reward: 476.47013705520624\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 162.891108\n",
            "---------------------------------------\n",
            "Total Timesteps: 5416 Episode Num: 6 Reward: 192.53324986910894\n",
            "Total Timesteps: 5441 Episode Num: 7 Reward: 4.927488075777364\n",
            "Total Timesteps: 6441 Episode Num: 8 Reward: 496.604965324709\n",
            "Total Timesteps: 6562 Episode Num: 9 Reward: 66.32680748014535\n",
            "Total Timesteps: 7562 Episode Num: 10 Reward: 486.600658920502\n",
            "Total Timesteps: 8562 Episode Num: 11 Reward: 469.9633453216879\n",
            "Total Timesteps: 9562 Episode Num: 12 Reward: 515.3359349057149\n",
            "Total Timesteps: 10562 Episode Num: 13 Reward: 475.2350031147499\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -9.256335\n",
            "---------------------------------------\n",
            "Total Timesteps: 11562 Episode Num: 14 Reward: -51.576918893157014\n",
            "Total Timesteps: 12562 Episode Num: 15 Reward: 99.56655340222338\n",
            "Total Timesteps: 13562 Episode Num: 16 Reward: 337.8678431052728\n",
            "Total Timesteps: 14562 Episode Num: 17 Reward: 133.3434002413861\n",
            "Total Timesteps: 14694 Episode Num: 18 Reward: 68.1823938505428\n",
            "Total Timesteps: 15694 Episode Num: 19 Reward: 365.20466270527464\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 203.936037\n",
            "---------------------------------------\n",
            "Total Timesteps: 16694 Episode Num: 20 Reward: 182.03058095422526\n",
            "Total Timesteps: 17694 Episode Num: 21 Reward: 210.67436055327613\n",
            "Total Timesteps: 18694 Episode Num: 22 Reward: 204.63369076973274\n",
            "Total Timesteps: 19694 Episode Num: 23 Reward: 418.8079580977111\n",
            "Total Timesteps: 20694 Episode Num: 24 Reward: 321.96654587025233\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 319.562438\n",
            "---------------------------------------\n",
            "Total Timesteps: 21694 Episode Num: 25 Reward: 289.8514189870221\n",
            "Total Timesteps: 22694 Episode Num: 26 Reward: 273.46954956482006\n",
            "Total Timesteps: 23694 Episode Num: 27 Reward: 191.0344065591631\n",
            "Total Timesteps: 24232 Episode Num: 28 Reward: 120.38939436726425\n",
            "Total Timesteps: 25232 Episode Num: 29 Reward: 320.19077312093407\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 236.716274\n",
            "---------------------------------------\n",
            "Total Timesteps: 26232 Episode Num: 30 Reward: 196.67842360289606\n",
            "Total Timesteps: 27232 Episode Num: 31 Reward: 187.17109322427822\n",
            "Total Timesteps: 27252 Episode Num: 32 Reward: 1.8876676083894537\n",
            "Total Timesteps: 27272 Episode Num: 33 Reward: 2.4819145777734257\n",
            "Total Timesteps: 27292 Episode Num: 34 Reward: 2.8094589874534472\n",
            "Total Timesteps: 27312 Episode Num: 35 Reward: 1.504359845724014\n",
            "Total Timesteps: 27332 Episode Num: 36 Reward: 2.1719821120835676\n",
            "Total Timesteps: 27352 Episode Num: 37 Reward: 3.8538485365660535\n",
            "Total Timesteps: 27372 Episode Num: 38 Reward: -0.10894874886387163\n",
            "Total Timesteps: 27393 Episode Num: 39 Reward: -1.0367896872412028\n",
            "Total Timesteps: 27413 Episode Num: 40 Reward: -0.6796504929649649\n",
            "Total Timesteps: 27434 Episode Num: 41 Reward: -0.806630994270749\n",
            "Total Timesteps: 27454 Episode Num: 42 Reward: 0.38006405901566387\n",
            "Total Timesteps: 27477 Episode Num: 43 Reward: -2.9891890999999324\n",
            "Total Timesteps: 27594 Episode Num: 44 Reward: 47.59412988363433\n",
            "Total Timesteps: 28594 Episode Num: 45 Reward: 278.8553224688232\n",
            "Total Timesteps: 29594 Episode Num: 46 Reward: 601.9139989144928\n",
            "Total Timesteps: 30594 Episode Num: 47 Reward: 322.8243348273984\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 333.892893\n",
            "---------------------------------------\n",
            "Total Timesteps: 31594 Episode Num: 48 Reward: 336.2712357636446\n",
            "Total Timesteps: 32594 Episode Num: 49 Reward: 324.7391616752817\n",
            "Total Timesteps: 33594 Episode Num: 50 Reward: 407.3844506280422\n",
            "Total Timesteps: 33614 Episode Num: 51 Reward: 3.7901962746267355\n",
            "Total Timesteps: 33636 Episode Num: 52 Reward: 4.5518553292757735\n",
            "Total Timesteps: 33657 Episode Num: 53 Reward: 3.6046579266130836\n",
            "Total Timesteps: 33677 Episode Num: 54 Reward: 3.0716099218478328\n",
            "Total Timesteps: 34677 Episode Num: 55 Reward: 422.7825046189127\n",
            "Total Timesteps: 35677 Episode Num: 56 Reward: 294.65835947316566\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2.666194\n",
            "---------------------------------------\n",
            "Total Timesteps: 35697 Episode Num: 57 Reward: 2.734472090891757\n",
            "Total Timesteps: 35717 Episode Num: 58 Reward: 3.359415290870979\n",
            "Total Timesteps: 35737 Episode Num: 59 Reward: 4.195833329262282\n",
            "Total Timesteps: 36737 Episode Num: 60 Reward: 312.30881172294517\n",
            "Total Timesteps: 37737 Episode Num: 61 Reward: 617.7372505392497\n",
            "Total Timesteps: 38737 Episode Num: 62 Reward: 585.6352202864053\n",
            "Total Timesteps: 39737 Episode Num: 63 Reward: 353.9825799677163\n",
            "Total Timesteps: 40737 Episode Num: 64 Reward: 509.0794167326185\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 350.926190\n",
            "---------------------------------------\n",
            "Total Timesteps: 41737 Episode Num: 65 Reward: 376.96818685781466\n",
            "Total Timesteps: 42737 Episode Num: 66 Reward: 622.4930958461991\n",
            "Total Timesteps: 43693 Episode Num: 67 Reward: 463.42981641627443\n",
            "Total Timesteps: 44693 Episode Num: 68 Reward: 502.7266083866132\n",
            "Total Timesteps: 45693 Episode Num: 69 Reward: 468.08403786494983\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 281.142331\n",
            "---------------------------------------\n",
            "Total Timesteps: 46693 Episode Num: 70 Reward: 357.2524897370277\n",
            "Total Timesteps: 47693 Episode Num: 71 Reward: 240.60148643721055\n",
            "Total Timesteps: 48693 Episode Num: 72 Reward: 202.55793450563735\n",
            "Total Timesteps: 49693 Episode Num: 73 Reward: 263.3536046886802\n",
            "Total Timesteps: 50693 Episode Num: 74 Reward: 379.1039228793072\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 290.003665\n",
            "---------------------------------------\n",
            "Total Timesteps: 51026 Episode Num: 75 Reward: 139.42097905487338\n",
            "Total Timesteps: 52026 Episode Num: 76 Reward: 503.9647983519857\n",
            "Total Timesteps: 53026 Episode Num: 77 Reward: 438.9320319242593\n",
            "Total Timesteps: 54026 Episode Num: 78 Reward: 522.1573841198\n",
            "Total Timesteps: 55019 Episode Num: 79 Reward: 453.4234893969931\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 571.260212\n",
            "---------------------------------------\n",
            "Total Timesteps: 56019 Episode Num: 80 Reward: 524.6921518825534\n",
            "Total Timesteps: 57019 Episode Num: 81 Reward: 404.6193980237446\n",
            "Total Timesteps: 57810 Episode Num: 82 Reward: 224.03871974701588\n",
            "Total Timesteps: 58810 Episode Num: 83 Reward: 310.95731376841013\n",
            "Total Timesteps: 59810 Episode Num: 84 Reward: 245.7352253297011\n",
            "Total Timesteps: 60675 Episode Num: 85 Reward: 177.9492474974405\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 179.021424\n",
            "---------------------------------------\n",
            "Total Timesteps: 60942 Episode Num: 86 Reward: 109.58223464563122\n",
            "Total Timesteps: 61942 Episode Num: 87 Reward: 605.341423456133\n",
            "Total Timesteps: 62942 Episode Num: 88 Reward: 537.2632120135231\n",
            "Total Timesteps: 63942 Episode Num: 89 Reward: 464.324665138683\n",
            "Total Timesteps: 64942 Episode Num: 90 Reward: 354.8210944993678\n",
            "Total Timesteps: 65942 Episode Num: 91 Reward: 249.33272977507812\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 329.905093\n",
            "---------------------------------------\n",
            "Total Timesteps: 66942 Episode Num: 92 Reward: 347.64700546313065\n",
            "Total Timesteps: 67942 Episode Num: 93 Reward: 195.51915556231924\n",
            "Total Timesteps: 68942 Episode Num: 94 Reward: 507.6569045842057\n",
            "Total Timesteps: 69942 Episode Num: 95 Reward: 215.68136966201098\n",
            "Total Timesteps: 70942 Episode Num: 96 Reward: 296.5219434956529\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 388.046875\n",
            "---------------------------------------\n",
            "Total Timesteps: 71942 Episode Num: 97 Reward: 466.81054566684674\n",
            "Total Timesteps: 72942 Episode Num: 98 Reward: 352.3905376564574\n",
            "Total Timesteps: 73942 Episode Num: 99 Reward: 309.4482157798685\n",
            "Total Timesteps: 74942 Episode Num: 100 Reward: 517.0303279249824\n",
            "Total Timesteps: 75942 Episode Num: 101 Reward: 422.36597670768265\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 593.128566\n",
            "---------------------------------------\n",
            "Total Timesteps: 76942 Episode Num: 102 Reward: 668.4498037244854\n",
            "Total Timesteps: 77942 Episode Num: 103 Reward: 654.0461580006888\n",
            "Total Timesteps: 78942 Episode Num: 104 Reward: 655.0349746153211\n",
            "Total Timesteps: 79942 Episode Num: 105 Reward: 521.8806524860911\n",
            "Total Timesteps: 80942 Episode Num: 106 Reward: 581.6147943479433\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 637.119052\n",
            "---------------------------------------\n",
            "Total Timesteps: 81942 Episode Num: 107 Reward: 652.050669301786\n",
            "Total Timesteps: 82942 Episode Num: 108 Reward: 606.2929608317813\n",
            "Total Timesteps: 83942 Episode Num: 109 Reward: 459.69517111161457\n",
            "Total Timesteps: 84942 Episode Num: 110 Reward: 714.9173579437177\n",
            "Total Timesteps: 85942 Episode Num: 111 Reward: 393.12137873884024\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 480.214848\n",
            "---------------------------------------\n",
            "Total Timesteps: 86942 Episode Num: 112 Reward: 506.1368179867875\n",
            "Total Timesteps: 87942 Episode Num: 113 Reward: 649.1237997376395\n",
            "Total Timesteps: 88942 Episode Num: 114 Reward: 299.10063210532945\n",
            "Total Timesteps: 89942 Episode Num: 115 Reward: 492.8394184844567\n",
            "Total Timesteps: 90942 Episode Num: 116 Reward: 552.2722386943689\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 506.791388\n",
            "---------------------------------------\n",
            "Total Timesteps: 91942 Episode Num: 117 Reward: 357.95083775885263\n",
            "Total Timesteps: 92942 Episode Num: 118 Reward: 655.2423905810457\n",
            "Total Timesteps: 93508 Episode Num: 119 Reward: 226.81569582928003\n",
            "Total Timesteps: 94188 Episode Num: 120 Reward: 297.3656567253899\n",
            "Total Timesteps: 95188 Episode Num: 121 Reward: 518.2858158152138\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 435.498031\n",
            "---------------------------------------\n",
            "Total Timesteps: 96188 Episode Num: 122 Reward: 633.6398226523341\n",
            "Total Timesteps: 97188 Episode Num: 123 Reward: 393.04772579277244\n",
            "Total Timesteps: 98188 Episode Num: 124 Reward: 462.43085256482726\n",
            "Total Timesteps: 99188 Episode Num: 125 Reward: 600.6085180409361\n",
            "Total Timesteps: 100188 Episode Num: 126 Reward: 645.6153412110313\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 546.043268\n",
            "---------------------------------------\n",
            "Total Timesteps: 101188 Episode Num: 127 Reward: 644.6617066846824\n",
            "Total Timesteps: 101684 Episode Num: 128 Reward: 219.53238256014907\n",
            "Total Timesteps: 101750 Episode Num: 129 Reward: 40.44800032465138\n",
            "Total Timesteps: 102097 Episode Num: 130 Reward: 194.65154991082827\n",
            "Total Timesteps: 103097 Episode Num: 131 Reward: 389.17418745308754\n",
            "Total Timesteps: 104097 Episode Num: 132 Reward: 237.65349818310335\n",
            "Total Timesteps: 104775 Episode Num: 133 Reward: 229.53065997680838\n",
            "Total Timesteps: 105535 Episode Num: 134 Reward: 390.20912302359187\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 367.120506\n",
            "---------------------------------------\n",
            "Total Timesteps: 106535 Episode Num: 135 Reward: 318.54530094100903\n",
            "Total Timesteps: 107535 Episode Num: 136 Reward: 526.9038407934925\n",
            "Total Timesteps: 108535 Episode Num: 137 Reward: 553.6978424014666\n",
            "Total Timesteps: 109535 Episode Num: 138 Reward: 540.5120781805741\n",
            "Total Timesteps: 110535 Episode Num: 139 Reward: 532.0007211788031\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 493.292129\n",
            "---------------------------------------\n",
            "Total Timesteps: 110706 Episode Num: 140 Reward: 79.18999765001044\n",
            "Total Timesteps: 111706 Episode Num: 141 Reward: 133.672539587573\n",
            "Total Timesteps: 112706 Episode Num: 142 Reward: 262.13445459892915\n",
            "Total Timesteps: 113706 Episode Num: 143 Reward: 425.5484807662652\n",
            "Total Timesteps: 114092 Episode Num: 144 Reward: 218.31242512788572\n",
            "Total Timesteps: 115092 Episode Num: 145 Reward: 639.0129476453438\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 255.727855\n",
            "---------------------------------------\n",
            "Total Timesteps: 115332 Episode Num: 146 Reward: 115.07116171116468\n",
            "Total Timesteps: 116332 Episode Num: 147 Reward: 511.48300114224537\n",
            "Total Timesteps: 117332 Episode Num: 148 Reward: 434.2044698310918\n",
            "Total Timesteps: 118332 Episode Num: 149 Reward: 172.6207809773259\n",
            "Total Timesteps: 119332 Episode Num: 150 Reward: 731.2887257118211\n",
            "Total Timesteps: 119671 Episode Num: 151 Reward: 113.45714608616423\n",
            "Total Timesteps: 120020 Episode Num: 152 Reward: 173.2031689787978\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 376.909135\n",
            "---------------------------------------\n",
            "Total Timesteps: 121020 Episode Num: 153 Reward: 652.9171354534759\n",
            "Total Timesteps: 122020 Episode Num: 154 Reward: 559.8916646998937\n",
            "Total Timesteps: 123020 Episode Num: 155 Reward: 606.4393792869378\n",
            "Total Timesteps: 124020 Episode Num: 156 Reward: 450.40409511488525\n",
            "Total Timesteps: 124673 Episode Num: 157 Reward: 302.3710770499158\n",
            "Total Timesteps: 125673 Episode Num: 158 Reward: 631.3380876147771\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 355.988663\n",
            "---------------------------------------\n",
            "Total Timesteps: 125956 Episode Num: 159 Reward: 138.5407269181237\n",
            "Total Timesteps: 126956 Episode Num: 160 Reward: 611.8073306023036\n",
            "Total Timesteps: 127956 Episode Num: 161 Reward: 420.66877135101623\n",
            "Total Timesteps: 128956 Episode Num: 162 Reward: 542.2296681307099\n",
            "Total Timesteps: 129956 Episode Num: 163 Reward: 616.2467587617183\n",
            "Total Timesteps: 130956 Episode Num: 164 Reward: 626.2546587225427\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 531.201111\n",
            "---------------------------------------\n",
            "Total Timesteps: 131956 Episode Num: 165 Reward: 643.4114354794082\n",
            "Total Timesteps: 132956 Episode Num: 166 Reward: 446.9342929573729\n",
            "Total Timesteps: 133956 Episode Num: 167 Reward: 402.0322283923511\n",
            "Total Timesteps: 134956 Episode Num: 168 Reward: 666.5585026107991\n",
            "Total Timesteps: 135956 Episode Num: 169 Reward: 561.8656278534054\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 577.000141\n",
            "---------------------------------------\n",
            "Total Timesteps: 136956 Episode Num: 170 Reward: 568.6102730240411\n",
            "Total Timesteps: 137373 Episode Num: 171 Reward: 237.6724243464791\n",
            "Total Timesteps: 137475 Episode Num: 172 Reward: 67.17165140251136\n",
            "Total Timesteps: 138475 Episode Num: 173 Reward: 557.6935887327359\n",
            "Total Timesteps: 139475 Episode Num: 174 Reward: 478.7963370675187\n",
            "Total Timesteps: 140475 Episode Num: 175 Reward: 609.5504447248861\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 421.766887\n",
            "---------------------------------------\n",
            "Total Timesteps: 141475 Episode Num: 176 Reward: 437.21388873242154\n",
            "Total Timesteps: 142475 Episode Num: 177 Reward: 515.7766639382071\n",
            "Total Timesteps: 143475 Episode Num: 178 Reward: 504.22857874295966\n",
            "Total Timesteps: 144475 Episode Num: 179 Reward: 485.4584629785074\n",
            "Total Timesteps: 145475 Episode Num: 180 Reward: 375.8091998495778\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 541.167804\n",
            "---------------------------------------\n",
            "Total Timesteps: 146475 Episode Num: 181 Reward: 640.7434684066938\n",
            "Total Timesteps: 147475 Episode Num: 182 Reward: 507.63124233773163\n",
            "Total Timesteps: 148475 Episode Num: 183 Reward: 241.75321098444283\n",
            "Total Timesteps: 149475 Episode Num: 184 Reward: 329.1668036153933\n",
            "Total Timesteps: 150475 Episode Num: 185 Reward: 535.4937006681329\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 482.623944\n",
            "---------------------------------------\n",
            "Total Timesteps: 151475 Episode Num: 186 Reward: 436.0758741343532\n",
            "Total Timesteps: 152475 Episode Num: 187 Reward: 596.4406635342906\n",
            "Total Timesteps: 153475 Episode Num: 188 Reward: 690.3673500261461\n",
            "Total Timesteps: 154475 Episode Num: 189 Reward: 603.6331592095121\n",
            "Total Timesteps: 155475 Episode Num: 190 Reward: 502.8843179212686\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 506.274824\n",
            "---------------------------------------\n",
            "Total Timesteps: 156475 Episode Num: 191 Reward: 555.4166369331836\n",
            "Total Timesteps: 157475 Episode Num: 192 Reward: 431.26041775441684\n",
            "Total Timesteps: 158475 Episode Num: 193 Reward: 717.6517560628411\n",
            "Total Timesteps: 159475 Episode Num: 194 Reward: 549.8548837151055\n",
            "Total Timesteps: 160475 Episode Num: 195 Reward: 694.9421996341923\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 515.680009\n",
            "---------------------------------------\n",
            "Total Timesteps: 161475 Episode Num: 196 Reward: 535.4473736419113\n",
            "Total Timesteps: 162475 Episode Num: 197 Reward: 367.55354225361395\n",
            "Total Timesteps: 163475 Episode Num: 198 Reward: 564.2516199912995\n",
            "Total Timesteps: 164475 Episode Num: 199 Reward: 589.5976846454314\n",
            "Total Timesteps: 165475 Episode Num: 200 Reward: 437.99446942322874\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 575.293469\n",
            "---------------------------------------\n",
            "Total Timesteps: 166475 Episode Num: 201 Reward: 461.8789451614304\n",
            "Total Timesteps: 167475 Episode Num: 202 Reward: 498.8395786819094\n",
            "Total Timesteps: 168475 Episode Num: 203 Reward: 748.3846530313936\n",
            "Total Timesteps: 169475 Episode Num: 204 Reward: 217.96870331585114\n",
            "Total Timesteps: 170475 Episode Num: 205 Reward: 588.4547631432966\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 499.815282\n",
            "---------------------------------------\n",
            "Total Timesteps: 171475 Episode Num: 206 Reward: 497.67102860945334\n",
            "Total Timesteps: 172475 Episode Num: 207 Reward: 481.5071932960239\n",
            "Total Timesteps: 173475 Episode Num: 208 Reward: 258.8221233012798\n",
            "Total Timesteps: 174475 Episode Num: 209 Reward: 530.2424900035295\n",
            "Total Timesteps: 175475 Episode Num: 210 Reward: 604.8678561401545\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 568.403920\n",
            "---------------------------------------\n",
            "Total Timesteps: 176475 Episode Num: 211 Reward: 719.2296242446293\n",
            "Total Timesteps: 177475 Episode Num: 212 Reward: 544.6122828233168\n",
            "Total Timesteps: 178475 Episode Num: 213 Reward: 484.54514348969485\n",
            "Total Timesteps: 179475 Episode Num: 214 Reward: 573.0254378302981\n",
            "Total Timesteps: 180475 Episode Num: 215 Reward: 689.6997507110891\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 584.532933\n",
            "---------------------------------------\n",
            "Total Timesteps: 181475 Episode Num: 216 Reward: 593.7066320952997\n",
            "Total Timesteps: 182475 Episode Num: 217 Reward: 744.7049750437667\n",
            "Total Timesteps: 183475 Episode Num: 218 Reward: 505.2357292497985\n",
            "Total Timesteps: 184475 Episode Num: 219 Reward: 496.7115446673634\n",
            "Total Timesteps: 185475 Episode Num: 220 Reward: 434.3381097136753\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 586.872221\n",
            "---------------------------------------\n",
            "Total Timesteps: 186475 Episode Num: 221 Reward: 646.3911813552904\n",
            "Total Timesteps: 187475 Episode Num: 222 Reward: 694.8288497696583\n",
            "Total Timesteps: 188475 Episode Num: 223 Reward: 758.5009470511337\n",
            "Total Timesteps: 189475 Episode Num: 224 Reward: 647.0863186572012\n",
            "Total Timesteps: 190475 Episode Num: 225 Reward: 673.9789615819724\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 586.000963\n",
            "---------------------------------------\n",
            "Total Timesteps: 191475 Episode Num: 226 Reward: 601.3241429531362\n",
            "Total Timesteps: 192475 Episode Num: 227 Reward: 772.2786752368271\n",
            "Total Timesteps: 193475 Episode Num: 228 Reward: 614.3830061472391\n",
            "Total Timesteps: 194475 Episode Num: 229 Reward: 653.1722375467663\n",
            "Total Timesteps: 195475 Episode Num: 230 Reward: 522.9237572479396\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 674.599219\n",
            "---------------------------------------\n",
            "Total Timesteps: 196475 Episode Num: 231 Reward: 592.3036110819054\n",
            "Total Timesteps: 197475 Episode Num: 232 Reward: 694.0170390067464\n",
            "Total Timesteps: 198475 Episode Num: 233 Reward: 691.8922803694853\n",
            "Total Timesteps: 199475 Episode Num: 234 Reward: 576.547161047434\n",
            "Total Timesteps: 200475 Episode Num: 235 Reward: 859.7841819859532\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 694.960869\n",
            "---------------------------------------\n",
            "Total Timesteps: 201475 Episode Num: 236 Reward: 707.3444952258826\n",
            "Total Timesteps: 202475 Episode Num: 237 Reward: 730.2637845380877\n",
            "Total Timesteps: 203475 Episode Num: 238 Reward: 597.9305352707638\n",
            "Total Timesteps: 204475 Episode Num: 239 Reward: 609.3176394377265\n",
            "Total Timesteps: 205475 Episode Num: 240 Reward: 553.2338336758544\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 645.820375\n",
            "---------------------------------------\n",
            "Total Timesteps: 206475 Episode Num: 241 Reward: 802.17711477878\n",
            "Total Timesteps: 207475 Episode Num: 242 Reward: 603.2274288522024\n",
            "Total Timesteps: 208475 Episode Num: 243 Reward: 600.2579308478986\n",
            "Total Timesteps: 209475 Episode Num: 244 Reward: 429.18638860267527\n",
            "Total Timesteps: 210475 Episode Num: 245 Reward: 484.7377371716371\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 676.890519\n",
            "---------------------------------------\n",
            "Total Timesteps: 211475 Episode Num: 246 Reward: 602.1015273894716\n",
            "Total Timesteps: 212475 Episode Num: 247 Reward: 774.9975523708947\n",
            "Total Timesteps: 213475 Episode Num: 248 Reward: 619.7596811340272\n",
            "Total Timesteps: 214475 Episode Num: 249 Reward: 812.6096167702511\n",
            "Total Timesteps: 215475 Episode Num: 250 Reward: 794.922430233703\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 661.004160\n",
            "---------------------------------------\n",
            "Total Timesteps: 216475 Episode Num: 251 Reward: 683.6622729347921\n",
            "Total Timesteps: 217475 Episode Num: 252 Reward: 783.7268920222589\n",
            "Total Timesteps: 218475 Episode Num: 253 Reward: 690.4857238688062\n",
            "Total Timesteps: 219475 Episode Num: 254 Reward: 871.3377867281265\n",
            "Total Timesteps: 220475 Episode Num: 255 Reward: 742.4211598533037\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 743.243346\n",
            "---------------------------------------\n",
            "Total Timesteps: 221475 Episode Num: 256 Reward: 866.0218911170591\n",
            "Total Timesteps: 222475 Episode Num: 257 Reward: 578.222260432015\n",
            "Total Timesteps: 223475 Episode Num: 258 Reward: 814.125034221011\n",
            "Total Timesteps: 224475 Episode Num: 259 Reward: 814.2536670191549\n",
            "Total Timesteps: 225475 Episode Num: 260 Reward: 743.4367719952844\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 706.925850\n",
            "---------------------------------------\n",
            "Total Timesteps: 226475 Episode Num: 261 Reward: 689.1393980056101\n",
            "Total Timesteps: 227475 Episode Num: 262 Reward: 839.2211791616679\n",
            "Total Timesteps: 228475 Episode Num: 263 Reward: 723.5292824626202\n",
            "Total Timesteps: 229475 Episode Num: 264 Reward: 728.8192295061768\n",
            "Total Timesteps: 230475 Episode Num: 265 Reward: 636.6520050118675\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 809.295708\n",
            "---------------------------------------\n",
            "Total Timesteps: 231475 Episode Num: 266 Reward: 903.8924467580072\n",
            "Total Timesteps: 232475 Episode Num: 267 Reward: 920.2778953933116\n",
            "Total Timesteps: 233475 Episode Num: 268 Reward: 929.0450930376834\n",
            "Total Timesteps: 234475 Episode Num: 269 Reward: 842.4680111583814\n",
            "Total Timesteps: 235475 Episode Num: 270 Reward: 677.0110297094504\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 743.738288\n",
            "---------------------------------------\n",
            "Total Timesteps: 236475 Episode Num: 271 Reward: 563.4575048873769\n",
            "Total Timesteps: 237475 Episode Num: 272 Reward: 383.7332207277032\n",
            "Total Timesteps: 238475 Episode Num: 273 Reward: 695.3437610223172\n",
            "Total Timesteps: 239475 Episode Num: 274 Reward: 771.5389052076608\n",
            "Total Timesteps: 240475 Episode Num: 275 Reward: 971.3809456286104\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 837.643720\n",
            "---------------------------------------\n",
            "Total Timesteps: 241475 Episode Num: 276 Reward: 877.7542133936727\n",
            "Total Timesteps: 242475 Episode Num: 277 Reward: 926.5311228313882\n",
            "Total Timesteps: 243475 Episode Num: 278 Reward: 859.2539676095926\n",
            "Total Timesteps: 244475 Episode Num: 279 Reward: 884.2182421968558\n",
            "Total Timesteps: 245475 Episode Num: 280 Reward: 814.4513490820028\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 793.076192\n",
            "---------------------------------------\n",
            "Total Timesteps: 246475 Episode Num: 281 Reward: 527.6011633472632\n",
            "Total Timesteps: 247475 Episode Num: 282 Reward: 943.341056793533\n",
            "Total Timesteps: 248475 Episode Num: 283 Reward: 773.747253926809\n",
            "Total Timesteps: 249475 Episode Num: 284 Reward: 923.3533445992758\n",
            "Total Timesteps: 250475 Episode Num: 285 Reward: 698.8767008747517\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 907.668840\n",
            "---------------------------------------\n",
            "Total Timesteps: 251475 Episode Num: 286 Reward: 995.2745603993765\n",
            "Total Timesteps: 252475 Episode Num: 287 Reward: 1014.5314210518894\n",
            "Total Timesteps: 253475 Episode Num: 288 Reward: 829.8224295234286\n",
            "Total Timesteps: 254475 Episode Num: 289 Reward: 716.7177837434239\n",
            "Total Timesteps: 255475 Episode Num: 290 Reward: 961.6495207947885\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 988.967893\n",
            "---------------------------------------\n",
            "Total Timesteps: 256475 Episode Num: 291 Reward: 1002.8987325741208\n",
            "Total Timesteps: 257475 Episode Num: 292 Reward: 747.2223003838857\n",
            "Total Timesteps: 258475 Episode Num: 293 Reward: 1094.6108133237483\n",
            "Total Timesteps: 259475 Episode Num: 294 Reward: 743.6723615009997\n",
            "Total Timesteps: 260475 Episode Num: 295 Reward: 1025.2111381967652\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 858.329306\n",
            "---------------------------------------\n",
            "Total Timesteps: 261475 Episode Num: 296 Reward: 709.5039186734514\n",
            "Total Timesteps: 262475 Episode Num: 297 Reward: 865.4402311283819\n",
            "Total Timesteps: 263475 Episode Num: 298 Reward: 972.0560240438865\n",
            "Total Timesteps: 264475 Episode Num: 299 Reward: 1350.6063753187643\n",
            "Total Timesteps: 265475 Episode Num: 300 Reward: 1374.234036074238\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1450.629699\n",
            "---------------------------------------\n",
            "Total Timesteps: 266475 Episode Num: 301 Reward: 1518.1668855650078\n",
            "Total Timesteps: 267475 Episode Num: 302 Reward: 1421.107073844355\n",
            "Total Timesteps: 268475 Episode Num: 303 Reward: 1568.4412599716532\n",
            "Total Timesteps: 269475 Episode Num: 304 Reward: 1422.8780438161218\n",
            "Total Timesteps: 270475 Episode Num: 305 Reward: 1073.8873420680693\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1350.595910\n",
            "---------------------------------------\n",
            "Total Timesteps: 271475 Episode Num: 306 Reward: 1352.121387412451\n",
            "Total Timesteps: 272475 Episode Num: 307 Reward: 1408.629229900887\n",
            "Total Timesteps: 273475 Episode Num: 308 Reward: 1487.8453902408555\n",
            "Total Timesteps: 274475 Episode Num: 309 Reward: 1352.7586704115788\n",
            "Total Timesteps: 275475 Episode Num: 310 Reward: 1347.9902457168516\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1446.004884\n",
            "---------------------------------------\n",
            "Total Timesteps: 276475 Episode Num: 311 Reward: 1475.783203153672\n",
            "Total Timesteps: 277475 Episode Num: 312 Reward: 1378.039407958254\n",
            "Total Timesteps: 278475 Episode Num: 313 Reward: 1417.277991993468\n",
            "Total Timesteps: 279475 Episode Num: 314 Reward: 1577.7612924147138\n",
            "Total Timesteps: 280475 Episode Num: 315 Reward: 1495.9325450153538\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1443.032104\n",
            "---------------------------------------\n",
            "Total Timesteps: 281475 Episode Num: 316 Reward: 1358.4886795583432\n",
            "Total Timesteps: 282475 Episode Num: 317 Reward: 1390.4070742905017\n",
            "Total Timesteps: 283475 Episode Num: 318 Reward: 1365.6788551805837\n",
            "Total Timesteps: 284475 Episode Num: 319 Reward: 1514.118540126475\n",
            "Total Timesteps: 285475 Episode Num: 320 Reward: 1446.4100437153995\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1596.172783\n",
            "---------------------------------------\n",
            "Total Timesteps: 286475 Episode Num: 321 Reward: 1440.1830426396548\n",
            "Total Timesteps: 287475 Episode Num: 322 Reward: 1677.5009544986608\n",
            "Total Timesteps: 288475 Episode Num: 323 Reward: 1597.222884855109\n",
            "Total Timesteps: 289475 Episode Num: 324 Reward: 1495.5382769213838\n",
            "Total Timesteps: 290475 Episode Num: 325 Reward: 1629.0074824050641\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1650.564722\n",
            "---------------------------------------\n",
            "Total Timesteps: 291475 Episode Num: 326 Reward: 1607.8528895699574\n",
            "Total Timesteps: 292475 Episode Num: 327 Reward: 1591.4414246452136\n",
            "Total Timesteps: 293475 Episode Num: 328 Reward: 1682.6756805816074\n",
            "Total Timesteps: 294475 Episode Num: 329 Reward: 1464.6265418819182\n",
            "Total Timesteps: 295475 Episode Num: 330 Reward: 1651.210682515148\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1654.116076\n",
            "---------------------------------------\n",
            "Total Timesteps: 296475 Episode Num: 331 Reward: 1654.8271871951133\n",
            "Total Timesteps: 297475 Episode Num: 332 Reward: 1699.3822360334455\n",
            "Total Timesteps: 298475 Episode Num: 333 Reward: 1684.0914501666093\n",
            "Total Timesteps: 299475 Episode Num: 334 Reward: 1697.5235841652225\n",
            "Total Timesteps: 300475 Episode Num: 335 Reward: 1788.6829986204725\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1671.427725\n",
            "---------------------------------------\n",
            "Total Timesteps: 301475 Episode Num: 336 Reward: 1719.0795192714525\n",
            "Total Timesteps: 302475 Episode Num: 337 Reward: 1785.469009507433\n",
            "Total Timesteps: 303475 Episode Num: 338 Reward: 1847.82098972736\n",
            "Total Timesteps: 304475 Episode Num: 339 Reward: 1761.6587109840186\n",
            "Total Timesteps: 305475 Episode Num: 340 Reward: 1780.8259021552174\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1882.542696\n",
            "---------------------------------------\n",
            "Total Timesteps: 306475 Episode Num: 341 Reward: 1913.5278189751098\n",
            "Total Timesteps: 307475 Episode Num: 342 Reward: 1948.4890997788598\n",
            "Total Timesteps: 308475 Episode Num: 343 Reward: 1798.224722491004\n",
            "Total Timesteps: 309475 Episode Num: 344 Reward: 1947.7231673093386\n",
            "Total Timesteps: 310475 Episode Num: 345 Reward: 1997.8675309610812\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1941.519217\n",
            "---------------------------------------\n",
            "Total Timesteps: 311475 Episode Num: 346 Reward: 2083.667590305949\n",
            "Total Timesteps: 312475 Episode Num: 347 Reward: 2005.277262020165\n",
            "Total Timesteps: 313475 Episode Num: 348 Reward: 2016.673876652798\n",
            "Total Timesteps: 314475 Episode Num: 349 Reward: 1843.4034346972421\n",
            "Total Timesteps: 315475 Episode Num: 350 Reward: 1998.440883893994\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2073.615014\n",
            "---------------------------------------\n",
            "Total Timesteps: 316475 Episode Num: 351 Reward: 1944.65000379332\n",
            "Total Timesteps: 317475 Episode Num: 352 Reward: 1941.9741553653132\n",
            "Total Timesteps: 318475 Episode Num: 353 Reward: 2016.4015439856137\n",
            "Total Timesteps: 319475 Episode Num: 354 Reward: 1938.687816796449\n",
            "Total Timesteps: 320475 Episode Num: 355 Reward: 1978.5937162445355\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1796.196299\n",
            "---------------------------------------\n",
            "Total Timesteps: 321475 Episode Num: 356 Reward: 1886.705690884044\n",
            "Total Timesteps: 322475 Episode Num: 357 Reward: 1742.7480960097498\n",
            "Total Timesteps: 323475 Episode Num: 358 Reward: 2072.5798005653487\n",
            "Total Timesteps: 324475 Episode Num: 359 Reward: 2003.9732393691675\n",
            "Total Timesteps: 325475 Episode Num: 360 Reward: 2006.591994293418\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1986.810829\n",
            "---------------------------------------\n",
            "Total Timesteps: 326475 Episode Num: 361 Reward: 2064.7768364557046\n",
            "Total Timesteps: 327475 Episode Num: 362 Reward: 2012.7847143249933\n",
            "Total Timesteps: 328475 Episode Num: 363 Reward: 1899.8318576322824\n",
            "Total Timesteps: 329475 Episode Num: 364 Reward: 2054.5421082344396\n",
            "Total Timesteps: 330475 Episode Num: 365 Reward: 1946.2306877646424\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1972.271450\n",
            "---------------------------------------\n",
            "Total Timesteps: 331475 Episode Num: 366 Reward: 1985.8946776682403\n",
            "Total Timesteps: 332475 Episode Num: 367 Reward: 1903.531303817764\n",
            "Total Timesteps: 333475 Episode Num: 368 Reward: 2016.5572971359145\n",
            "Total Timesteps: 334475 Episode Num: 369 Reward: 1969.4170104170123\n",
            "Total Timesteps: 335475 Episode Num: 370 Reward: 2073.8174704586495\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1961.741402\n",
            "---------------------------------------\n",
            "Total Timesteps: 336475 Episode Num: 371 Reward: 2006.6740385250291\n",
            "Total Timesteps: 337475 Episode Num: 372 Reward: 2045.112542925773\n",
            "Total Timesteps: 338475 Episode Num: 373 Reward: 1726.825322428541\n",
            "Total Timesteps: 339475 Episode Num: 374 Reward: 2021.1350637759222\n",
            "Total Timesteps: 340475 Episode Num: 375 Reward: 1969.7579011737253\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1879.781091\n",
            "---------------------------------------\n",
            "Total Timesteps: 341475 Episode Num: 376 Reward: 1862.3270380652987\n",
            "Total Timesteps: 342475 Episode Num: 377 Reward: 1883.7512199994976\n",
            "Total Timesteps: 343475 Episode Num: 378 Reward: 1917.9865247578036\n",
            "Total Timesteps: 344475 Episode Num: 379 Reward: 1973.552859060924\n",
            "Total Timesteps: 345475 Episode Num: 380 Reward: 1923.4064908353944\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1954.013742\n",
            "---------------------------------------\n",
            "Total Timesteps: 346475 Episode Num: 381 Reward: 1939.3494671935398\n",
            "Total Timesteps: 347475 Episode Num: 382 Reward: 1575.452953568398\n",
            "Total Timesteps: 348475 Episode Num: 383 Reward: 2013.4851456191843\n",
            "Total Timesteps: 349475 Episode Num: 384 Reward: 2100.0320435305252\n",
            "Total Timesteps: 350475 Episode Num: 385 Reward: 1730.7451177405803\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1953.590096\n",
            "---------------------------------------\n",
            "Total Timesteps: 351475 Episode Num: 386 Reward: 1992.2639445767397\n",
            "Total Timesteps: 352475 Episode Num: 387 Reward: 2015.834301509152\n",
            "Total Timesteps: 353475 Episode Num: 388 Reward: 1946.1786519078069\n",
            "Total Timesteps: 354475 Episode Num: 389 Reward: 1982.7797399062633\n",
            "Total Timesteps: 355475 Episode Num: 390 Reward: 2135.9660729446477\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2027.335203\n",
            "---------------------------------------\n",
            "Total Timesteps: 356475 Episode Num: 391 Reward: 2105.6269939745916\n",
            "Total Timesteps: 357475 Episode Num: 392 Reward: 2012.7418796684533\n",
            "Total Timesteps: 358475 Episode Num: 393 Reward: 2177.0365752699695\n",
            "Total Timesteps: 359475 Episode Num: 394 Reward: 2122.470584645433\n",
            "Total Timesteps: 360475 Episode Num: 395 Reward: 2090.3806927114733\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2148.058682\n",
            "---------------------------------------\n",
            "Total Timesteps: 361475 Episode Num: 396 Reward: 2086.6872169855183\n",
            "Total Timesteps: 362475 Episode Num: 397 Reward: 2136.0846914886365\n",
            "Total Timesteps: 363475 Episode Num: 398 Reward: 2057.1556220782786\n",
            "Total Timesteps: 364475 Episode Num: 399 Reward: 2062.722844439513\n",
            "Total Timesteps: 365475 Episode Num: 400 Reward: 2156.700525365195\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2116.841747\n",
            "---------------------------------------\n",
            "Total Timesteps: 366475 Episode Num: 401 Reward: 1992.0562111987956\n",
            "Total Timesteps: 367475 Episode Num: 402 Reward: 2198.9916406875795\n",
            "Total Timesteps: 368475 Episode Num: 403 Reward: 2248.778050081254\n",
            "Total Timesteps: 369475 Episode Num: 404 Reward: 2123.428791471376\n",
            "Total Timesteps: 370475 Episode Num: 405 Reward: 2200.1713004770863\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2244.658432\n",
            "---------------------------------------\n",
            "Total Timesteps: 371475 Episode Num: 406 Reward: 2163.9049930795295\n",
            "Total Timesteps: 372475 Episode Num: 407 Reward: 2239.4735057287257\n",
            "Total Timesteps: 373475 Episode Num: 408 Reward: 2222.1607304561226\n",
            "Total Timesteps: 374475 Episode Num: 409 Reward: 2170.018730271547\n",
            "Total Timesteps: 375475 Episode Num: 410 Reward: 2030.1281686917343\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2167.360974\n",
            "---------------------------------------\n",
            "Total Timesteps: 376475 Episode Num: 411 Reward: 2171.3870935912323\n",
            "Total Timesteps: 377475 Episode Num: 412 Reward: 2222.7040126255697\n",
            "Total Timesteps: 378475 Episode Num: 413 Reward: 2162.927196299971\n",
            "Total Timesteps: 379475 Episode Num: 414 Reward: 2178.741008908514\n",
            "Total Timesteps: 380475 Episode Num: 415 Reward: 2083.1644468027935\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2229.763013\n",
            "---------------------------------------\n",
            "Total Timesteps: 381475 Episode Num: 416 Reward: 2098.6672824348516\n",
            "Total Timesteps: 382475 Episode Num: 417 Reward: 2171.6661717214965\n",
            "Total Timesteps: 383475 Episode Num: 418 Reward: 2172.0729450003455\n",
            "Total Timesteps: 384475 Episode Num: 419 Reward: 2172.435707961981\n",
            "Total Timesteps: 385475 Episode Num: 420 Reward: 2143.310050989383\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2244.680402\n",
            "---------------------------------------\n",
            "Total Timesteps: 386475 Episode Num: 421 Reward: 2275.1624803646073\n",
            "Total Timesteps: 387475 Episode Num: 422 Reward: 2113.545620557135\n",
            "Total Timesteps: 388475 Episode Num: 423 Reward: 2164.778327276672\n",
            "Total Timesteps: 389475 Episode Num: 424 Reward: 2174.2178589735336\n",
            "Total Timesteps: 390475 Episode Num: 425 Reward: 2190.546399349102\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2295.987567\n",
            "---------------------------------------\n",
            "Total Timesteps: 391475 Episode Num: 426 Reward: 2282.137077848196\n",
            "Total Timesteps: 392475 Episode Num: 427 Reward: 2107.3472553470456\n",
            "Total Timesteps: 393475 Episode Num: 428 Reward: 2190.608265934671\n",
            "Total Timesteps: 394475 Episode Num: 429 Reward: 2150.7026100310027\n",
            "Total Timesteps: 395475 Episode Num: 430 Reward: 2093.5398831706298\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2266.643276\n",
            "---------------------------------------\n",
            "Total Timesteps: 396475 Episode Num: 431 Reward: 2230.762058830765\n",
            "Total Timesteps: 397475 Episode Num: 432 Reward: 2145.399396546812\n",
            "Total Timesteps: 398475 Episode Num: 433 Reward: 2121.6578296365155\n",
            "Total Timesteps: 399475 Episode Num: 434 Reward: 2203.73983076407\n",
            "Total Timesteps: 400475 Episode Num: 435 Reward: 2150.9272219291965\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2140.168070\n",
            "---------------------------------------\n",
            "Total Timesteps: 401475 Episode Num: 436 Reward: 2063.526974091598\n",
            "Total Timesteps: 402475 Episode Num: 437 Reward: 2225.2594497978516\n",
            "Total Timesteps: 403475 Episode Num: 438 Reward: 2261.6528897981243\n",
            "Total Timesteps: 404475 Episode Num: 439 Reward: 2159.6748184351354\n",
            "Total Timesteps: 405475 Episode Num: 440 Reward: 2181.7971512073873\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2221.326811\n",
            "---------------------------------------\n",
            "Total Timesteps: 406475 Episode Num: 441 Reward: 2180.456882146796\n",
            "Total Timesteps: 407475 Episode Num: 442 Reward: 2084.850893138589\n",
            "Total Timesteps: 408475 Episode Num: 443 Reward: 2246.534271387901\n",
            "Total Timesteps: 409475 Episode Num: 444 Reward: 2105.9674477220856\n",
            "Total Timesteps: 410475 Episode Num: 445 Reward: 2307.6120185009636\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2181.662905\n",
            "---------------------------------------\n",
            "Total Timesteps: 411475 Episode Num: 446 Reward: 2193.0058620564505\n",
            "Total Timesteps: 412475 Episode Num: 447 Reward: 2092.617660482734\n",
            "Total Timesteps: 413475 Episode Num: 448 Reward: 2282.441358038705\n",
            "Total Timesteps: 414475 Episode Num: 449 Reward: 2241.4001618498105\n",
            "Total Timesteps: 415475 Episode Num: 450 Reward: 2172.9914601129312\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2243.035673\n",
            "---------------------------------------\n",
            "Total Timesteps: 416475 Episode Num: 451 Reward: 2214.5931196227734\n",
            "Total Timesteps: 417475 Episode Num: 452 Reward: 2295.1986359731104\n",
            "Total Timesteps: 418475 Episode Num: 453 Reward: 2259.7771909086214\n",
            "Total Timesteps: 419475 Episode Num: 454 Reward: 2259.843129397029\n",
            "Total Timesteps: 420475 Episode Num: 455 Reward: 2350.592972387215\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2249.627119\n",
            "---------------------------------------\n",
            "Total Timesteps: 421475 Episode Num: 456 Reward: 2275.341653741479\n",
            "Total Timesteps: 422475 Episode Num: 457 Reward: 2218.949993520056\n",
            "Total Timesteps: 423475 Episode Num: 458 Reward: 2196.695431588859\n",
            "Total Timesteps: 424475 Episode Num: 459 Reward: 2127.680916477754\n",
            "Total Timesteps: 425475 Episode Num: 460 Reward: 2266.98463844319\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2352.999464\n",
            "---------------------------------------\n",
            "Total Timesteps: 426475 Episode Num: 461 Reward: 2379.208634221987\n",
            "Total Timesteps: 427475 Episode Num: 462 Reward: 2193.0823870095373\n",
            "Total Timesteps: 428475 Episode Num: 463 Reward: 2284.1460360885712\n",
            "Total Timesteps: 429475 Episode Num: 464 Reward: 2252.0105275755777\n",
            "Total Timesteps: 430475 Episode Num: 465 Reward: 2309.3110904836103\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2367.638514\n",
            "---------------------------------------\n",
            "Total Timesteps: 431475 Episode Num: 466 Reward: 2359.0439626984853\n",
            "Total Timesteps: 432475 Episode Num: 467 Reward: 2308.387299258103\n",
            "Total Timesteps: 433475 Episode Num: 468 Reward: 2348.834645737942\n",
            "Total Timesteps: 434475 Episode Num: 469 Reward: 2409.2686698968364\n",
            "Total Timesteps: 435475 Episode Num: 470 Reward: 2284.7412799700064\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2253.918856\n",
            "---------------------------------------\n",
            "Total Timesteps: 436475 Episode Num: 471 Reward: 2243.4566512289284\n",
            "Total Timesteps: 437475 Episode Num: 472 Reward: 2281.786629908744\n",
            "Total Timesteps: 438475 Episode Num: 473 Reward: 2352.359533386509\n",
            "Total Timesteps: 439475 Episode Num: 474 Reward: 2371.356839138449\n",
            "Total Timesteps: 440475 Episode Num: 475 Reward: 2304.1999092451974\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2400.805405\n",
            "---------------------------------------\n",
            "Total Timesteps: 441475 Episode Num: 476 Reward: 2356.126249185924\n",
            "Total Timesteps: 442475 Episode Num: 477 Reward: 2336.9032823196785\n",
            "Total Timesteps: 443475 Episode Num: 478 Reward: 2283.7811402125426\n",
            "Total Timesteps: 444475 Episode Num: 479 Reward: 2404.4215323702224\n",
            "Total Timesteps: 445475 Episode Num: 480 Reward: 2375.935541660735\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2359.865668\n",
            "---------------------------------------\n",
            "Total Timesteps: 446475 Episode Num: 481 Reward: 2285.7366556983516\n",
            "Total Timesteps: 447475 Episode Num: 482 Reward: 2419.655707965926\n",
            "Total Timesteps: 448475 Episode Num: 483 Reward: 2309.002161856579\n",
            "Total Timesteps: 449475 Episode Num: 484 Reward: 2326.855398525352\n",
            "Total Timesteps: 450475 Episode Num: 485 Reward: 2242.0993183027263\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2237.795541\n",
            "---------------------------------------\n",
            "Total Timesteps: 451475 Episode Num: 486 Reward: 2206.0281002359807\n",
            "Total Timesteps: 452475 Episode Num: 487 Reward: 2387.1373172541007\n",
            "Total Timesteps: 453475 Episode Num: 488 Reward: 2288.979309822315\n",
            "Total Timesteps: 454475 Episode Num: 489 Reward: 2297.1646164764056\n",
            "Total Timesteps: 455475 Episode Num: 490 Reward: 2296.8777419197972\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2368.170504\n",
            "---------------------------------------\n",
            "Total Timesteps: 456475 Episode Num: 491 Reward: 2306.5346073249566\n",
            "Total Timesteps: 457475 Episode Num: 492 Reward: 2234.8061302726287\n",
            "Total Timesteps: 458475 Episode Num: 493 Reward: 2423.512193647535\n",
            "Total Timesteps: 459475 Episode Num: 494 Reward: 2320.7534571270144\n",
            "Total Timesteps: 460475 Episode Num: 495 Reward: 2334.5661488761593\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2248.239629\n",
            "---------------------------------------\n",
            "Total Timesteps: 461475 Episode Num: 496 Reward: 2215.905369420735\n",
            "Total Timesteps: 462475 Episode Num: 497 Reward: 2346.0135885932327\n",
            "Total Timesteps: 463475 Episode Num: 498 Reward: 2393.1032249322493\n",
            "Total Timesteps: 464475 Episode Num: 499 Reward: 2290.5382015496916\n",
            "Total Timesteps: 465475 Episode Num: 500 Reward: 2312.319953505582\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2389.853924\n",
            "---------------------------------------\n",
            "Total Timesteps: 466475 Episode Num: 501 Reward: 2437.4495081761547\n",
            "Total Timesteps: 467475 Episode Num: 502 Reward: 2313.4173930392867\n",
            "Total Timesteps: 468475 Episode Num: 503 Reward: 2206.8675910013894\n",
            "Total Timesteps: 469475 Episode Num: 504 Reward: 2370.233508570185\n",
            "Total Timesteps: 470475 Episode Num: 505 Reward: 2442.7626088974425\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2279.712048\n",
            "---------------------------------------\n",
            "Total Timesteps: 471475 Episode Num: 506 Reward: 2198.9765813477466\n",
            "Total Timesteps: 472475 Episode Num: 507 Reward: 2270.076648935959\n",
            "Total Timesteps: 473475 Episode Num: 508 Reward: 2396.932814682699\n",
            "Total Timesteps: 474475 Episode Num: 509 Reward: 2385.5366499522406\n",
            "Total Timesteps: 475475 Episode Num: 510 Reward: 2401.6724141127593\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2420.183026\n",
            "---------------------------------------\n",
            "Total Timesteps: 476475 Episode Num: 511 Reward: 2229.2512698227947\n",
            "Total Timesteps: 477475 Episode Num: 512 Reward: 2490.3396506576532\n",
            "Total Timesteps: 478475 Episode Num: 513 Reward: 2237.2110994957006\n",
            "Total Timesteps: 479475 Episode Num: 514 Reward: 2338.073982360016\n",
            "Total Timesteps: 480475 Episode Num: 515 Reward: 2224.42100197419\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2427.446708\n",
            "---------------------------------------\n",
            "Total Timesteps: 481475 Episode Num: 516 Reward: 2432.309723646787\n",
            "Total Timesteps: 482475 Episode Num: 517 Reward: 2107.272299104431\n",
            "Total Timesteps: 483475 Episode Num: 518 Reward: 2325.2692143985287\n",
            "Total Timesteps: 484475 Episode Num: 519 Reward: 2207.599625582432\n",
            "Total Timesteps: 485475 Episode Num: 520 Reward: 2052.267141689108\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2398.798382\n",
            "---------------------------------------\n",
            "Total Timesteps: 486475 Episode Num: 521 Reward: 2375.486708972066\n",
            "Total Timesteps: 487475 Episode Num: 522 Reward: 2227.517876132105\n",
            "Total Timesteps: 488475 Episode Num: 523 Reward: 2065.7545071461304\n",
            "Total Timesteps: 489475 Episode Num: 524 Reward: 2365.7586278743656\n",
            "Total Timesteps: 490475 Episode Num: 525 Reward: 2192.5488912947108\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2363.024098\n",
            "---------------------------------------\n",
            "Total Timesteps: 491475 Episode Num: 526 Reward: 2295.3377940834066\n",
            "Total Timesteps: 492475 Episode Num: 527 Reward: 2337.4338242991357\n",
            "Total Timesteps: 493475 Episode Num: 528 Reward: 2078.145311578043\n",
            "Total Timesteps: 494475 Episode Num: 529 Reward: 2173.513965278246\n",
            "Total Timesteps: 495475 Episode Num: 530 Reward: 2041.6368367999567\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2411.835916\n",
            "---------------------------------------\n",
            "Total Timesteps: 496475 Episode Num: 531 Reward: 2330.007971750964\n",
            "Total Timesteps: 497475 Episode Num: 532 Reward: 2434.0211683001953\n",
            "Total Timesteps: 498475 Episode Num: 533 Reward: 2302.845095590118\n",
            "Total Timesteps: 499475 Episode Num: 534 Reward: 2422.1665076581007\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2466.211064\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4d1YAMqif1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "f3a01890-3703-44fa-dfc4-bf8bd461b5ee"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2451.348971\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLqoXDW9jQua",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9ef33493-3523-4efc-ebeb-bb768bdcfa83"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJdcN7BOjioi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r '/content/results' '/content/drive/My Drive/App/RL/Session10/Att3'\n",
        "!cp -r '/content/pytorch_models' '/content/drive/My Drive/App/RL/Session10/Att3'\n",
        "!cp -r '/content/exp' '/content/drive/My Drive/App/RL/Session10/Att3'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKBPPG9Wjwqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}