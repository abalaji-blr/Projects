{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Glove Pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective: Use pretrained GloVe (Global Vectors ) and find out the analogies and nearest neighbours.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained vectors are availabe at the following websites.\n",
    "\n",
    "#### GloVe: https://nlp.stanford.edu/projects/glove/\n",
    "#### Direct link: http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#### Git hub: https://github.com/stanfordnlp/GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.2\n"
     ]
    }
   ],
   "source": [
    "print np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GloVe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glove.6B.100d.txt',\n",
       " 'glove.6B.200d.txt',\n",
       " 'glove.6B.300d.txt',\n",
       " 'glove.6B.50d.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/Users/abalaji/myData/NLP/Glove/glove.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\r\n"
     ]
    }
   ],
   "source": [
    "# different versions of global vectors are available : Dimensions 50, 100, 200 etc.\n",
    "# let's load the smaller footprint\n",
    "# it's a text file.\n",
    "!head -1 '/Users/abalaji/myData/NLP/Glove/glove.6B/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set up the globals for GloVe\n",
    "word2vec = {} # dictionary\n",
    "embedding = []\n",
    "idx2word  = []   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Done\n",
      "Length of dictionary 400000\n"
     ]
    }
   ],
   "source": [
    "print('Loading word vectors...')\n",
    "# basically, a word followed by the vector related to it.\n",
    "dataLocation = '/Users/abalaji/myData/NLP/Glove/glove.6B/glove.6B.50d.txt'\n",
    "with open(dataLocation) as f:\n",
    "    for line in f:\n",
    "        # format is word bunch of vectors\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        vector = np.asarray(tokens[1:], dtype='float32')\n",
    "        #\n",
    "        # add to globals\n",
    "        word2vec[word] = vector\n",
    "        embedding.append(vector)\n",
    "        idx2word.append(word)\n",
    "        \n",
    "        \n",
    "print('Done')\n",
    "print('Length of dictionary %s' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = np.array(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VocabSize, Dim = embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 50\n"
     ]
    }
   ],
   "source": [
    "print VocabSize, Dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# for a given vector, find out the nearest one (cosine distance) by iterating thru\n",
    "# the global variable - embedding\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 1 0 4 5]\n"
     ]
    }
   ],
   "source": [
    "# about np.argsort()\n",
    "print np.argsort(np.array([3, 1, 0, -1, 5, 6]))\n",
    "\n",
    "# returns the index to sorted array in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in the case of word analogies, 3 words are given, we have to find out the fourth one.\n",
    "def find_analogies(w1, w2, w3):\n",
    "    # sanity check\n",
    "    for word in (w1, w2, w3):\n",
    "        if word not in word2vec:\n",
    "            print('%s not found' % word)\n",
    "            return\n",
    "        \n",
    "    # process\n",
    "    # get the vectors for the given words\n",
    "    # use popular analogy to name the variable\n",
    "    king = word2vec[w1]\n",
    "    man  = word2vec[w2]\n",
    "    woman = word2vec[w3]\n",
    "    # note that vector is nothing but a bunch of numbers (based on dim)\n",
    "    input_vector = king - man + woman\n",
    "    \n",
    "    # find out the closest vector\n",
    "    # want the outut in the shape of global variable - 'embedding', so that \n",
    "    # we can index it after sorting\n",
    "    distances = pairwise_distances(input_vector.reshape(1, Dim), embedding, metric='cosine').reshape(VocabSize)\n",
    "    #print('distances shape %d' % len(distances))\n",
    "    #print(distances)\n",
    "    \n",
    "    # get the index using np.argsort()\n",
    "    # get only the top 4, we are looking for the fourth word.\n",
    "    indices = distances.argsort()[:4] \n",
    "    for index in indices:\n",
    "        # get the word\n",
    "        word = idx2word[index]\n",
    "        \n",
    "        if word not in (w1, w2, w3):\n",
    "            best_word = word\n",
    "            break\n",
    "    \n",
    "    print('%10s - %10s = %10s - %10s' % (w1,  w2, w3, best_word) )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      king -        man =      woman -      queen\n",
      "    france -      paris =     london -    britain\n",
      "    france -      paris =       rome -      italy\n",
      "     paris -     france =      italy -       rome\n",
      "    france -     french =    english -    england\n",
      "     japan -   japanese =    chinese -      china\n",
      "     japan -   japanese =    italian -      italy\n",
      "     japan -   japanese = australian -  australia\n",
      "  december -   november =       june -       july\n",
      "     miami -    florida =      texas -    houston\n",
      "  einstein -  scientist =    painter -    matisse\n",
      "     china -       rice =      bread -    chinese\n",
      "       man -      woman =        she -         he\n",
      "       man -      woman =       aunt -      uncle\n",
      "       man -      woman =     sister -    brother\n",
      "       man -      woman =       wife -     friend\n",
      "       man -      woman =    actress -      actor\n",
      "       man -      woman =     mother -     father\n",
      "      heir -    heiress =   princess -      queen\n",
      "    nephew -      niece =       aunt -      uncle\n",
      "    france -      paris =      tokyo -      japan\n",
      "    france -      paris =    beijing -      china\n",
      "  february -    january =   november -    october\n",
      "    france -      paris =       rome -      italy\n",
      "     paris -     france =      italy -       rome\n"
     ]
    }
   ],
   "source": [
    "find_analogies('king', 'man', 'woman')\n",
    "find_analogies('france', 'paris', 'london')\n",
    "find_analogies('france', 'paris', 'rome')\n",
    "find_analogies('paris', 'france', 'italy')\n",
    "find_analogies('france', 'french', 'english')\n",
    "find_analogies('japan', 'japanese', 'chinese')\n",
    "find_analogies('japan', 'japanese', 'italian')\n",
    "find_analogies('japan', 'japanese', 'australian')\n",
    "find_analogies('december', 'november', 'june')\n",
    "find_analogies('miami', 'florida', 'texas')\n",
    "find_analogies('einstein', 'scientist', 'painter')\n",
    "find_analogies('china', 'rice', 'bread')\n",
    "find_analogies('man', 'woman', 'she')\n",
    "find_analogies('man', 'woman', 'aunt')\n",
    "find_analogies('man', 'woman', 'sister')\n",
    "find_analogies('man', 'woman', 'wife')\n",
    "find_analogies('man', 'woman', 'actress')\n",
    "find_analogies('man', 'woman', 'mother')\n",
    "find_analogies('heir', 'heiress', 'princess')\n",
    "find_analogies('nephew', 'niece', 'aunt')\n",
    "find_analogies('france', 'paris', 'tokyo')\n",
    "find_analogies('france', 'paris', 'beijing')\n",
    "find_analogies('february', 'january', 'november')\n",
    "find_analogies('france', 'paris', 'rome')\n",
    "find_analogies('paris', 'france', 'italy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
