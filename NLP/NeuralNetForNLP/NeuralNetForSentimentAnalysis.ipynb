{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net For Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    " Use Neural Net to find out the sentiment of a movie (Cornell Movie Dataset) based on review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Background**: \n",
    "\n",
    "* With LogisticRegression model, we have got the accuracy of about 84%. [Refer to this python notebook](./MovieSentiment.ipynb)\n",
    "\n",
    "* Let's see check the sentiment accuracy using Neural Network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the steps involved to use Neural Network.\n",
    "\n",
    "  * [Data preparation](#read_data)\n",
    "      + **Get the Vocabulary set.**\n",
    "      + **Split the data into train and test. Use only train data to build Vocabulary set.**\n",
    "  \n",
    "  * [Train CNN with Embedding Layer](#nnet)\n",
    "  \n",
    "  * Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='read_data'></a>\n",
    "### Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = load_files(\"./txt_sentoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['target_names', 'data', 'target', 'DESCR', 'filenames']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "print reviews.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print reviews.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the data and target values\n",
    "X, y = reviews.data, reviews.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary set using Train data\n",
    "\n",
    "* Clean the train data and collect the often used words.\n",
    "* Make sure to ignore the stop_words (often used sight words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the given input text\n",
    "def cleanup_text(text):\n",
    "    clean_tokens = []\n",
    "    # conver to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove special characters\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    \n",
    "    # ignore single character words\n",
    "    text = re.sub('^\\s+[a-z]\\s+$', ' ', text)\n",
    "    \n",
    "    # ignore more spaces\n",
    "    text = re.sub('^\\s+$', ' ', text)\n",
    "    \n",
    "    # ignore the stop words\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        if word not in stopwords.words('english'):\n",
    "            clean_tokens.append(word)\n",
    "        \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1234', 'hi', 'movie', 'nice']\n"
     ]
    }
   ],
   "source": [
    "print(cleanup_text('1234 hi this movie is nice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '1234', '10', '30', 'boring', 'good', 'idea']\n"
     ]
    }
   ],
   "source": [
    "print(cleanup_text('hello 1234 !@#@ 10:30 very boring !!!!. Not a good idea !@!#$ '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Often used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt in range(len(X_train)):\n",
    "    tokens = cleanup_text(X_train[cnt])\n",
    "    Vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36164"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 7666), ('movie', 4673), ('one', 4669), ('like', 2942), ('even', 2056), ('good', 1962), ('time', 1896), ('story', 1747), ('would', 1701), ('much', 1625), ('character', 1622), ('also', 1588), ('get', 1580), ('well', 1548), ('two', 1512), ('characters', 1485), ('first', 1475), ('see', 1411), ('way', 1332), ('make', 1321), ('really', 1255), ('life', 1231), ('plot', 1228), ('little', 1214), ('films', 1209)]\n"
     ]
    }
   ],
   "source": [
    "print(Vocab.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider tokens only where words occur atleast twice\n",
    "min_occurrence = 2\n",
    "Tokens = [k for k,c in Vocab.items() if c >= min_occurrence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23075\n"
     ]
    }
   ],
   "source": [
    "print(len(Tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Vocab list to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(lines, file_name):\n",
    "    # get the data in chunk\n",
    "    data = '\\n'.join(lines)\n",
    "    \n",
    "    f = open(file_name, 'w')\n",
    "    f.write(data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocab tokens to vocab.txt\n",
    "save_to_file(Tokens, 'Vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nnet'></a>\n",
    "## Build Embedding Layer\n",
    "\n",
    "* [Load the Vocabulary.](#load_vocab)\n",
    "* [Encode the training data to numbers (using tokenizer)](#encode_train)\n",
    "* Use Keras Embedding Layer\n",
    "* Build CNN model\n",
    "* Save the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_vocab'></a>\n",
    "#### Load Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the vocabulary\n",
    "def read_vocabulary_file(file_name):\n",
    "    file = open(file_name, 'r')\n",
    "    text = file.read()\n",
    "    return text\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary file\n",
    "Vocab = read_vocabulary_file('./Vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = set(Vocab.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23075\n"
     ]
    }
   ],
   "source": [
    "print(len(Vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the train data\n",
    "\n",
    "For each training review, clean the review and make sure to drop the words if they are NOT in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the review\n",
    "# return the cleaned words as one string\n",
    "def cleanup_review(text):\n",
    "    # conver to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove special characters\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    \n",
    "    # ignore single character words\n",
    "    text = re.sub('^\\s+[a-z]\\s+$', ' ', text)\n",
    "    \n",
    "    # split into tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    #filter out the tokens\n",
    "    clean_tokens = [word for word in tokens if word in Vocab]\n",
    "    clean_tokens = ' '.join(clean_tokens)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thrilling movie\n"
     ]
    }
   ],
   "source": [
    "print( cleanup_review(\"that was a thrilling movie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt in range(len(X_train)):\n",
    "    clean_text = cleanup_review(X_train[cnt])\n",
    "    X_train_clean.append(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='encode_train'></a>\n",
    "#### Encode the Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x1c24adf550>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tokenizer.texts_to_matrix of <keras_preprocessing.text.Tokenizer object at 0x1c24adf550>>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23027\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23027\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_encoded_mat = tokenizer.texts_to_matrix(X_train_clean, mode='count')\n",
    "X_train_encoded_mat = tokenizer.texts_to_sequences(X_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_encoded_mat[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_encoded_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all words in vocab + 1 for unknown word\n",
    "vocab_size = len(tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23028"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the maximum number of words in a review\n",
    "max_review_length = max([len(s.split()) for s in X_train_clean ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_review_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already the train data is encoded.\n",
    "\n",
    "# keras prefers each input should be of same length.\n",
    "# pad each input to max_review_length\n",
    "X_train_encoded_padded = pad_sequences(X_train_encoded_mat, maxlen=max_review_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1274,  365, 2332, ...,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train_encoded_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train_encoded_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Neural Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    # vocab size\n",
    "    # dimensions = 100\n",
    "    # input length\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_review_length))\n",
    "    # add the convolution and max pooling layers\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu' ))\n",
    "    model.add(Dense(1, activation='sigmoid' ))\n",
    "    # compile network\n",
    "    model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    #plot_model(model, to_file= 'model.png' , show_shapes=True)\n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 1192, 100)         2302800   \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1185, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                189450    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,517,893\n",
      "Trainable params: 2,517,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./model.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 22s - loss: 0.6880 - acc: 0.5369\n",
      "Epoch 2/10\n",
      " - 22s - loss: 0.4450 - acc: 0.8706\n",
      "Epoch 3/10\n",
      " - 22s - loss: 0.0469 - acc: 0.9994\n",
      "Epoch 4/10\n",
      " - 22s - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 22s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 23s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 432s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 23s - loss: 9.2943e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 23s - loss: 7.8380e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 22s - loss: 6.7627e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c287fe090>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "model.fit(X_train_encoded_padded, y_train, epochs=10, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('model.h5' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='metric'></a>\n",
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Translate the test data\n",
    "X_test_clean = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the test data\n",
    "for cnt in range(len(X_test)):\n",
    "    clean_text = cleanup_review(X_test[cnt])\n",
    "    X_test_clean.append(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tokenizer to encode the test data\n",
    "X_test_encoded_mat = tokenizer.texts_to_sequences(X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_encoded_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encoded_padded = pad_sequences(X_test_encoded_mat, maxlen=max_review_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the test data\n",
    "\n",
    "_, acc = model.evaluate(X_test_encoded_padded, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.000000\n"
     ]
    }
   ],
   "source": [
    "print( 'Test Accuracy: %f'  % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
